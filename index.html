<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>S M Kamrul Hasan ‚Äì Senior AI Scientist at Johnson & Johnson</title>

  <meta name="description" content="S M Kamrul Hasan is a Senior AI Scientist at Johnson & Johnson, working on computer vision, medical imaging, and AI/ML. PhD in Imaging Science (RIT).">
  <link rel="canonical" href="https://smkamrulhasan.github.io/">
  <link rel="icon" href="myIcon.ico">

  <meta name="google-site-verification" content="X2QFrl-bPeg9AdlMt4VKT9v6MJUSTCf-SrY3CvKt4Zs">

  <!-- Social preview -->
  <meta property="og:title" content="S M Kamrul Hasan ‚Äì Senior AI Scientist at Johnson & Johnson">
  <meta property="og:description" content="Computer vision, medical imaging, and AI/ML. PhD in Imaging Science (RIT).">
  <meta property="og:url" content="https://smkamrulhasan.github.io/">
  <meta property="og:type" content="website">

  <link rel="stylesheet" href="jemdoc.css" type="text/css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&display=swap">

  <style>
    body { font-family: system-ui, -apple-system, BlinkMacSystemFont, "SF Pro Text", "SF Pro Display", "Inter", "Segoe UI", Arial, sans-serif; }

    .typing-container{
      font-size: 34px;
      font-weight: 500;
      color: #2563eb;
      margin: 0 0 10px 0;
      letter-spacing: 0.2px;
      line-height: 1.2;
    }
    .cursor{
      display:inline-block;
      margin-left: 2px;
      animation: blink 1s infinite;
    }
    @keyframes blink { 0%,50%,100%{opacity:1} 25%,75%{opacity:0} }

    /* ===== About + Research Interests ===== */
    .about-wrap{ margin: 10px 0 30px 0; }
    .about-grid{
      display: grid;
      grid-template-columns: 1.2fr 0.8fr;
      gap: 18px;
      align-items: start;
    }
    .about-card{
      border: 1px solid #e6e6e6;
      border-radius: 14px;
      padding: 16px 16px 14px 16px;
      background: #fff;
      box-shadow: 0 6px 18px rgba(0,0,0,0.04);
    }
    .about-title{
      margin: 0 0 8px 0;
      font-size: 28px;
      font-weight: 800;
      letter-spacing: -0.2px;
      color: #111;
    }
    .about-sub{
      margin: 0 0 14px 0;
      font-size: 15px;
      color: #444;
      line-height: 1.55;
    }
    .about-sub a{ text-decoration: none; }
    .about-sub a:hover{ text-decoration: underline; }

    .pill-row{ margin: 10px 0 0 0; }
    .pill{
      display: inline-block;
      padding: 6px 12px;
      margin: 6px 8px 0 0;
      border-radius: 999px;
      border: 1px solid #ddd;
      background: #f7f7f7;
      font-size: 13px;
      color: #222;
      cursor: pointer;
      user-select: none;
      transition: transform 120ms ease, background 120ms ease, border 120ms ease;
    }
    .pill:hover{ transform: translateY(-1px); }
    .pill.active{ background: #111; color: #fff; border-color: #111; }

    .hl{
      padding: 0 4px;
      border-radius: 6px;
      background: rgba(253, 108, 108, 0.22);
    }

    details.about-details{
      margin-top: 12px;
      border-top: 1px dashed #e6e6e6;
      padding-top: 10px;
    }
    details.about-details summary{
      cursor: pointer;
      font-weight: 700;
      color: #111;
      list-style: none;
    }
    details.about-details summary::-webkit-details-marker{ display: none; }
    details.about-details summary:before{ content: "‚ûï "; font-weight: 700; }
    details.about-details[open] summary:before{ content: "‚ûñ "; }

    .mini-list{ margin: 0; padding-left: 18px; }
    .mini-list li{ margin: 8px 0; line-height: 1.45; color: #222; }

    @media (max-width: 860px){
      .about-grid{ grid-template-columns: 1fr; }
    }
  </style>

  <!-- Structured data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Person",
    "name": "S M Kamrul Hasan",
    "jobTitle": "Senior AI Scientist",
    "worksFor": { "@type": "Organization", "name": "Johnson & Johnson" },
    "url": "https://smkamrulhasan.github.io/",
    "sameAs": [
      "https://www.linkedin.com/in/s-m-kamrul-hasan/",
      "https://github.com/SMKamrulHasan",
      "https://scholar.google.com/citations?user=M7XmUK0AAAAJ"
    ]
  }
  </script>

  <!-- Google Analytics (optional) -->
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-159069803-1', 'auto');
  ga('send', 'pageview');
  </script>
</head>

<body>

<div id="layout-content" style="margin-top:25px">

  <a href="https://github.com/SMKamrulHasan" class="github-corner" aria-label="GitHub profile">
    <svg width="80" height="80" viewBox="0 0 250 250" style="fill:#FD6C6C; color:#fff; position:absolute; top:0; right:0; border:0;">
      <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
      <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
            fill="currentColor" style="transform-origin:130px 106px;" class="octo-arm"></path>
      <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
            fill="currentColor" class="octo-body"></path>
    </svg>
  </a>

  <style>
    .github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}
    @keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}
    @media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}
  </style>

  <table>
    <tbody>
      <tr>
        <td width="650">
          <div id="toptitle">
            <h1 style="color:black;">S M Kamrul Hasan</h1>

            <div class="typing-container">
              <span id="typing-text"></span><span class="cursor">|</span>
            </div>

            <p style="padding-bottom:30px;">
              Senior AI Scientist<br>
              Johnson &amp; Johnson<br><br>
              Email: sh3190@rit.edu<br>
              Previous:
              <a target="_blank" href="https://www.philips.com/a-w/about/innovation/innovation-hubs/cambridge.html" rel="noopener">Philips Research</a>;
              <a target="_blank" href="http://www.research.ibm.com/" rel="noopener">IBM Research</a><br>
              [<a target="_blank" href="data/SMKamrul_Hasan_Resume_singlePage.pdf" rel="noopener">CV</a>]
              [<a target="_blank" href="https://scholar.google.com/citations?user=M7XmUK0AAAAJ&hl=en" rel="noopener">Google Scholar</a>]
            </p>

            <p>
              <a href="https://scholar.google.com/citations?user=M7XmUK0AAAAJ&hl=en" rel="noopener">
                <img src="./images/gs-logo.png" height="30" style="margin-bottom:-3px" alt="Google Scholar">
              </a>
              <a href="https://github.com/SMKamrulHasan" rel="noopener">
                <img src="./pic/github_s.jpg" height="30" style="margin-bottom:-3px" alt="GitHub">
              </a>
              <a href="https://www.researchgate.net/profile/S_M_Kamrul_Hasan7" rel="noopener">
                <img src="./pic/rg.png" height="30" style="margin-bottom:-3px" alt="ResearchGate">
              </a>
              <a href="https://www.linkedin.com/in/s-m-kamrul-hasan/" rel="noopener">
                <img src="./images/linkedin-logo.png" height="30" style="margin-bottom:-3px" alt="LinkedIn">
              </a>
            </p>
          </div>
        </td>
        <td>
          <img src="./pic/git_image.jpg" border="0" width="260" alt="S M Kamrul Hasan"><br>
        </td>
      </tr>
    </tbody>
  </table>

  <!-- ===== About + Research Interests ===== -->
  <div class="about-wrap">
    <div class="about-grid">

      <div class="about-card">
        <h2 class="about-title">About Me</h2>

        <p class="about-sub" id="aboutText">
          I‚Äôm a Senior AI Scientist at Johnson &amp; Johnson working on computer vision, medical imaging, and AI/ML.
          I earned my PhD from
          <a href="https://www.cis.rit.edu/" target="_blank" rel="noopener">Chester F. Carlson Center for Imaging Science</a>
          at Rochester Institute of Technology (RIT) under
          <a href="https://www.rit.edu/directory/calbme-cristian-linte" target="_blank" rel="noopener">Dr. Cristian Linte</a>.
          My research has focused on <span data-tag="Segmentation">segmentation</span>, <span data-tag="Detection">detection</span>,
          and <span data-tag="Optimization">model optimization</span> in real-world imaging systems.
        </p>

        <div class="pill-row" aria-label="Interactive focus tags">
          <span class="pill" data-focus="Segmentation">Segmentation</span>
          <span class="pill" data-focus="Detection">Detection</span>
          <span class="pill" data-focus="Optimization">Optimization</span>
          <span class="pill" data-focus="GenAI">GenAI</span>
          <span class="pill" data-focus="Uncertainty">Uncertainty</span>
        </div>

        <details class="about-details">
          <summary>Thesis (one line)</summary>
          <p class="about-sub" style="margin-top:10px;">
            Semi-supervised multi-task deep learning for robust clinical segmentation.
          </p>
        </details>
      </div>

      <div class="about-card">
        <h2 class="about-title">Research Interests</h2>
        <ul class="mini-list" id="riList">
          <li><span data-tag="Segmentation">Semantic &amp; instance segmentation</span> (2D/3D medical imaging)</li>
          <li><span data-tag="Registration">4D deformable registration</span> + motion estimation</li>
          <li><span data-tag="Detection">Object detection</span> + representation learning</li>
          <li><span data-tag="Uncertainty">Uncertainty / OOD estimation</span> + Bayesian methods</li>
          <li><span data-tag="Inpainting">Video inpainting</span> + generative modeling</li>
          <li><span data-tag="GenAI">LLM/GenAI workflows</span> for automation &amp; information retrieval</li>
        </ul>

        <details class="about-details" open>
          <summary>What I enjoy building</summary>
          <p class="about-sub" style="margin-top:10px;">
            End-to-end systems: data ‚Üí model ‚Üí evaluation ‚Üí deployment, with reliability and measurable impact.
          </p>
        </details>
      </div>

    </div>
  </div>

</div>

<!-- Typing script -->
<script>
(function () {
  var text = "Let‚Äôs dive into my research";
  var speed = 60;
  var i = 0;

  function type() {
    var el = document.getElementById("typing-text");
    if (!el) return;
    if (i < text.length) {
      el.appendChild(document.createTextNode(text.charAt(i)));
      i++;
      window.setTimeout(type, speed);
    }
  }

  if (document.addEventListener) {
    document.addEventListener("DOMContentLoaded", function () {
      window.setTimeout(type, 300);
    });
  } else {
    window.attachEvent("onload", function () {
      window.setTimeout(type, 300);
    });
  }
})();
</script>

<script>
(function(){
  const pills = document.querySelectorAll(".pill");
  const targets = document.querySelectorAll("[data-tag]");
  let active = null;

  function clearHighlights(){
    targets.forEach(el => el.classList.remove("hl"));
    pills.forEach(p => p.classList.remove("active"));
  }

  pills.forEach(pill => {
    pill.addEventListener("click", () => {
      const focus = pill.getAttribute("data-focus");
      if(active === focus){
        active = null;
        clearHighlights();
        return;
      }
      active = focus;
      clearHighlights();
      pill.classList.add("active");
      targets.forEach(el => {
        if(el.getAttribute("data-tag") === focus){
          el.classList.add("hl");
        }
      });
    });
  });
})();
</script>

</body>
</html>
<!-- ===== End section ===== -->


<!-- ===== Recent AI Contributions + Selected Contributions (Interactive) ===== -->
<style>
  /* Reuse your existing card look; only adds the interactive bits */
  .contrib-wrap{ margin: 0 0 30px 0; }

  .contrib-grid{
    display: grid;
    grid-template-columns: 1fr 1fr;
    gap: 18px;
    align-items: start;
  }
  @media (max-width: 860px){
    .contrib-grid{ grid-template-columns: 1fr; }
  }

  /* Make pills consistent with your About section */
  .contrib-pills{ margin: 10px 0 0 0; }
  .contrib-pill{
    display: inline-block;
    padding: 6px 12px;
    margin: 6px 8px 0 0;
    border-radius: 999px;
    border: 1px solid #ddd;
    background: #f7f7f7;
    font-size: 13px;
    color: #222;
    cursor: pointer;
    user-select: none;
    transition: transform 120ms ease, background 120ms ease, border 120ms ease;
  }
  .contrib-pill:hover{ transform: translateY(-1px); }
  .contrib-pill.active{
    background: #111;
    color: #fff;
    border-color: #111;
  }

  /* highlight effect */
  .contrib-hl{
    padding: 0 4px;
    border-radius: 6px;
    background: rgba(37, 99, 235, 0.16); /* soft blue */
  }

  /* optional: small ‚Äúhint‚Äù line */
  .contrib-hint{
    margin-top: 10px;
    font-size: 13px;
    color: #666;
  }
</style>

<div class="contrib-wrap">

  <!-- Pills row (place it exactly where you want: above the two cards, or below) -->
  <div class="contrib-pills" aria-label="Interactive contribution filters">
    <span class="contrib-pill" data-focus="Student‚ÄìTeacher Framework">Student‚ÄìTeacher Framework</span>
    <span class="contrib-pill" data-focus="Video Inpainting">Video Inpainting</span>
    <span class="contrib-pill" data-focus="Pruning">Pruning</span>
    <span class="contrib-pill" data-focus="GenAI">GenAI</span>
    <span class="contrib-pill" data-focus="Vision Transformer">Vision Transformer</span>
    <span class="contrib-pill" data-focus="Camera Imaging">Camera Imaging</span>
  </div>
  <div class="contrib-hint">Tip: click a pill to highlight related items across both panels.</div>

  <div class="contrib-grid">

    <!-- Left card: Recent AI Contributions (use your existing .about-card style if you want) -->
    <div class="about-card">
      <h2 class="about-title">Recent AI Contributions</h2>

      <ul class="mini-list" style="margin-top: 6px;">
        <li>
          <b><span data-tag="Vision Transformer">Vision Transformers</span> for Lung Cancer Detection</b> ‚Äî
          Developed and deployed transformer-based imaging models at J&amp;J to improve lung cancer detection performance and streamline clinical decision workflows.
          <span data-tag="Systems" style="display:none;"></span>
        </li>

        <li>
          <b><span data-tag="GenAI">LLM-powered</span> Drug Discovery Insights</b> ‚Äî
          Built NLP pipelines (BERT/GPT-style) to analyze multi-channel biomedical text and signals, improving discovery of high-signal insights for downstream decision-making.
        </li>
      </ul>

      <div style="margin-top: 14px; padding: 12px 14px; border: 1px dashed #e6e6e6; border-radius: 12px; background: #fff7f7;">
        <b>What I optimize for:</b> reliability, measurable lift, and deployability (not just prototypes).
        <span data-tag="Systems"></span>
        <span data-tag="Optimization"></span>
      </div>
    </div>

    <!-- Right card: Selected Contributions -->
    <div class="about-card">
      <h2 class="about-title">Selected Contributions</h2>

      <div style="margin-top: 8px;">
        <div style="font-weight:800; font-size:18px; margin: 0 0 6px 0;">
          üéì <span data-tag="Student‚ÄìTeacher Framework">STAMP</span>
        </div>
        <div class="about-sub" style="margin:0 0 14px 0;">
          Developed a self-training <span data-tag="Student‚ÄìTeacher Framework">student‚Äìteacher framework</span> for precise medical image segmentation.
        </div>

        <div style="font-weight:800; font-size:18px; margin: 0 0 6px 0;">
          üß† <span data-tag="Pruning">CondenseUNet</span>
        </div>
        <div class="about-sub" style="margin:0 0 14px 0;">
          Engineered compression, pruning, and distillation methods to reduce model footprint, reducing medical model sizes by 70% while maintaining 99% accuracy. This method is now integral to the reasoning models utilized by DeepSeek-R1 for efficient inference.
        </div>

        <div style="font-weight:800; font-size:18px; margin: 0 0 6px 0;">
          ‚ú® <span data-tag="Video Inpainting">Surgical Occlusion Inpainting</span>
        </div>
        <div class="about-sub" style="margin:0;">
          Built vision-based inpainting to remove laparoscopic tool occlusions from surgical video sequences. üöÄ Demonstrating an early example of what later evolved into commercial features like <span data-tag="Camera Imaging">Google Pixel Camera‚Äôs Magic Eraser</span> for content-aware object removal.
        </div>

        <!-- Optional invisible tag hooks -->
        <span data-tag="Systems" style="display:none;"></span>
      </div>
    </div>

  </div>
</div>

<script>
  // --- Interactive highlighting for Contributions ---
  (function(){
    var pills = document.querySelectorAll(".contrib-pill");
    var targets = document.querySelectorAll(".contrib-wrap [data-tag]");
    var active = null;

    function clearAll(){
      for (var i=0; i<targets.length; i++){
        targets[i].classList.remove("contrib-hl");
      }
      for (var j=0; j<pills.length; j++){
        pills[j].classList.remove("active");
      }
    }

    for (var k=0; k<pills.length; k++){
      (function(pill){
        pill.addEventListener("click", function(){
          var focus = pill.getAttribute("data-focus");

          // toggle off
          if (active === focus){
            active = null;
            clearAll();
            return;
          }

          active = focus;
          clearAll();
          pill.classList.add("active");

          for (var t=0; t<targets.length; t++){
            if (targets[t].getAttribute("data-tag") === focus){
              targets[t].classList.add("contrib-hl");
            }
          }
        });
      })(pills[k]);
    }
  })();
</script>


				
<!-- ===== News (Timeline + Filters + Show More) ===== -->
<style>
  .news-wrap{
    font-family: Aptos, Arial, sans-serif;
    margin: 10px 0 30px 0;
  }

  .news-head{
    display:flex;
    align-items:baseline;
    justify-content:space-between;
    gap: 12px;
    margin-bottom: 10px;
  }
  .news-title{
    margin: 0;
    font-size: 28px;
    font-weight: 800;
    letter-spacing: -0.2px;
    color: #111;
  }
  .news-sub{
    margin: 0;
    font-size: 14px;
    color: #666;
  }

  .news-card{
    border: 1px solid #e6e6e6;
    border-radius: 14px;
    padding: 14px 14px 10px 14px;
    background: #fff;
    box-shadow: 0 6px 18px rgba(0,0,0,0.04);
  }

  /* filter chips */
  .news-filters{ margin: 10px 0 10px 0; }
  .chip{
    display:inline-block;
    padding: 6px 12px;
    margin: 6px 8px 0 0;
    border-radius: 999px;
    border: 1px solid #ddd;
    background: #f7f7f7;
    font-size: 13px;
    color:#222;
    cursor:pointer;
    user-select:none;
    transition: transform 120ms ease, background 120ms ease, border 120ms ease;
  }
  .chip:hover{ transform: translateY(-1px); }
  .chip.active{
    background:#111;
    color:#fff;
    border-color:#111;
  }

  /* timeline */
  .timeline{ margin: 0; padding: 0; list-style: none; }
  .t-item{
    display:grid;
    grid-template-columns: 92px 18px 1fr;
    gap: 10px;
    padding: 4px 6px;          /* tighter spacing */
    border-radius: 10px;
  }
  .t-item:hover{ background: #fafafa; }

  .t-date{
    font-weight: 700;
    color:#1f4bb8;
    white-space: nowrap;
    font-size: 14px;
  }
  .t-dotcol{
    position: relative;
    display:flex;
    justify-content:center;
  }
  .t-dot{
    width: 10px;
    height: 10px;
    border-radius: 50%;
    background: #FD6C6C;
    margin-top: 2px;           /* tighter */
  }
  .t-line{
    position:absolute;
    top: 16px;
    bottom: -6px;              /* tighter */
    width: 2px;
    background: #eee;
  }

  .t-text{
    line-height: 1.45;
    color:#111;
  }
  .t-text a{ color:#1f4bb8; text-decoration:none; }
  .t-text a:hover{ text-decoration:underline; }

  /* actions */
  .news-actions{
    display:flex;
    gap: 10px;
    margin-top: 10px;
  }
  .news-btn{
    border: 1px solid #ddd;
    background: #fff;
    padding: 8px 12px;
    border-radius: 10px;
    font-family: inherit;
    font-size: 13px;
    cursor:pointer;
  }
  .news-btn:hover{ background:#f7f7f7; }

  /* hidden items (default collapsed list) */
  .is-hidden{ display:none; }

  @media (max-width: 700px){
    .t-item{ grid-template-columns: 86px 16px 1fr; }
  }
</style>

<div class="news-wrap">
  <div class="news-head">
    <h2 class="news-title">News</h2>
    <p class="news-sub">Filter by year ‚Ä¢ Expand for more</p>
  </div>

  <div class="news-card">

    <div class="news-filters" id="newsFilters">
      <span class="chip active" data-year="all">All</span>
      <span class="chip" data-year="2024">2024</span>
      <span class="chip" data-year="2023">2023</span>
      <span class="chip" data-year="2022">2022</span>
      <span class="chip" data-year="2021">2021</span>
      <span class="chip" data-year="2020">2020</span>
      <span class="chip" data-year="2019">2019</span>
    </div>

    <ul class="timeline" id="newsTimeline">

      <!-- Visible by default -->
      <li class="t-item" data-year="2024">
        <div class="t-date">Jan 2024</div>
        <div class="t-dotcol"><span class="t-dot"></span><span class="t-line"></span></div>
        <div class="t-text">Promoted as <b>Senior AI Scientist</b></div>
      </li>

      <li class="t-item" data-year="2023">
        <div class="t-date">Feb 2023</div>
        <div class="t-dotcol"><span class="t-dot"></span><span class="t-line"></span></div>
        <div class="t-text">Joined <b>Johnson & Johnson</b> as AI Scientist</div>
      </li>

      <li class="t-item" data-year="2023">
        <div class="t-date">Jan 2023</div>
        <div class="t-dotcol"><span class="t-dot"></span><span class="t-line"></span></div>
        <div class="t-text">Successfully defended my <b>PhD Dissertation</b>.</div>
      </li>

      <li class="t-item" data-year="2022">
        <div class="t-date">May 2022</div>
        <div class="t-dotcol"><span class="t-dot"></span><span class="t-line"></span></div>
        <div class="t-text">One paper got accepted to <b>MIUA 2022</b>.</div>
      </li>

      <li class="t-item" data-year="2022">
        <div class="t-date">May 2022</div>
        <div class="t-dotcol"><span class="t-dot"></span><span class="t-line"></span></div>
        <div class="t-text">Accepted to <b>Oxford ML Summer School</b> (~5% acceptance rate).</div>
      </li>

      <li class="t-item" data-year="2022">
        <div class="t-date">Mar 2022</div>
        <div class="t-dotcol"><span class="t-dot"></span><span class="t-line"></span></div>
        <div class="t-text">Successfully passed my <b>Ph.D. Candidacy Exam</b>.</div>
      </li>

      <!-- Hidden initially (Show more reveals in All view) -->
      <li class="t-item is-hidden" data-year="2021">
        <div class="t-date">Oct 2021</div>
        <div class="t-dotcol"><span class="t-dot"></span><span class="t-line"></span></div>
        <div class="t-text">
          Attended the <a href="https://researchsummit.microsoft.com/home_public" target="_blank">Microsoft Research Summit 2021</a>.
        </div>
      </li>

      <li class="t-item is-hidden" data-year="2021">
        <div class="t-date">Aug 2021</div>
        <div class="t-dotcol"><span class="t-dot"></span><span class="t-line"></span></div>
        <div class="t-text">
          Started Research Internship at
          <a href="https://www.philips.com/a-w/research/locations/cambridge-north-america.html" target="_blank">Philips</a>,
          Research North America, Cambridge, MA, USA.
        </div>
      </li>

      <li class="t-item is-hidden" data-year="2021">
        <div class="t-date">May 2021</div>
        <div class="t-dotcol"><span class="t-dot"></span><span class="t-line"></span></div>
        <div class="t-text">Got accepted to UCL Medical Image Computing Summer School (MedICSS), London.</div>
      </li>

      <li class="t-item is-hidden" data-year="2021">
        <div class="t-date">Mar 2021</div>
        <div class="t-dotcol"><span class="t-dot"></span><span class="t-line"></span></div>
        <div class="t-text">
          Accepted offer as AI Research Intern at
          <a href="https://www.usa.philips.com/" target="_blank">Philips Research</a>, Cambridge, Massachusetts.
        </div>
      </li>

      <li class="t-item is-hidden" data-year="2020">
        <div class="t-date">Nov 2020</div>
        <div class="t-dotcol"><span class="t-dot"></span><span class="t-line"></span></div>
        <div class="t-text">Started Winter School at Center for Computational Medicine in Cardiology, Switzerland.</div>
      </li>

      <li class="t-item is-hidden" data-year="2020">
        <div class="t-date">Oct 2020</div>
        <div class="t-dotcol"><span class="t-dot"></span><span class="t-line"></span></div>
        <div class="t-text">
          Paper got accepted at
          <a href="https://spie.org/conferences-and-exhibitions/medical-imaging?SSO=1" target="_blank">SPIE 2021</a>,
          San Diego, California.
        </div>
      </li>

      <li class="t-item is-hidden" data-year="2020">
        <div class="t-date">Aug 2020</div>
        <div class="t-dotcol"><span class="t-dot"></span><span class="t-line"></span></div>
        <div class="t-text">Received <b>MICCAI Student Award</b> as a part of NSF grant.</div>
      </li>

      <li class="t-item is-hidden" data-year="2020">
        <div class="t-date">Aug 2020</div>
        <div class="t-dotcol"><span class="t-dot"></span><span class="t-line"></span></div>
        <div class="t-text">
          Started Research Internship at
          <a href="https://www.research.ibm.com/" target="_blank">IBM</a>, Almaden Research Center, San Jose, California.
        </div>
      </li>

      <li class="t-item is-hidden" data-year="2020">
        <div class="t-date">May 2020</div>
        <div class="t-dotcol"><span class="t-dot"></span><span class="t-line"></span></div>
        <div class="t-text">
          Accepted offer as Research Intern at
          <a href="https://www.ibm.com/us-en/" target="_blank">IBM</a>, San Jose, California.
        </div>
      </li>

      <li class="t-item is-hidden" data-year="2020">
        <div class="t-date">Apr 2020</div>
        <div class="t-dotcol"><span class="t-dot"></span><span class="t-line"></span></div>
        <div class="t-text">Presented paper at ISBI 2020.</div>
      </li>

      <li class="t-item is-hidden" data-year="2020">
        <div class="t-date">Feb 2020</div>
        <div class="t-dotcol"><span class="t-dot"></span><span class="t-line"></span></div>
        <div class="t-text">Reviewer for MICCAI 2020.</div>
      </li>

      <li class="t-item is-hidden" data-year="2019">
        <div class="t-date">Nov 2019</div>
        <div class="t-dotcol"><span class="t-dot"></span><span class="t-line"></span></div>
        <div class="t-text">U-NetPlus paper accepted for oral presentation at RIT Graduate Showcase 2019.</div>
      </li>

    </ul>

    <div class="news-actions">
      <button class="news-btn" id="toggleMoreBtn">Show more</button>
      <button class="news-btn" id="clearFilterBtn" title="Reset filters">Reset</button>
    </div>

  </div>
</div>

<script>
(function(){
  const chips = Array.from(document.querySelectorAll("#newsFilters .chip"));
  const items = Array.from(document.querySelectorAll("#newsTimeline .t-item"));
  const toggleBtn = document.getElementById("toggleMoreBtn");
  const resetBtn  = document.getElementById("clearFilterBtn");

  let showAll = false;      // show hidden items in "All"
  let yearFilter = "all";   // selected year

  function apply(){
    items.forEach(li => {
      const y = li.getAttribute("data-year");
      const hiddenByDefault = li.classList.contains("is-hidden");

      const passYear = (yearFilter === "all") || (y === yearFilter);

      // ‚úÖ Key fix:
      // If a specific year is selected, show those items even if is-hidden.
      const passMore = (yearFilter !== "all") ? true : (showAll ? true : !hiddenByDefault);

      li.style.display = (passYear && passMore) ? "" : "none";
    });

    // Hide show-more button when filtering by a year (not needed)
    if(yearFilter !== "all"){
      toggleBtn.style.display = "none";
    }else{
      toggleBtn.style.display = "";
      toggleBtn.textContent = showAll ? "Show less" : "Show more";
    }
  }

  chips.forEach(chip => {
    chip.addEventListener("click", () => {
      chips.forEach(c => c.classList.remove("active"));
      chip.classList.add("active");
      yearFilter = chip.getAttribute("data-year");
      apply();
    });
  });

  toggleBtn.addEventListener("click", () => {
    showAll = !showAll;
    apply();
  });

  resetBtn.addEventListener("click", () => {
    yearFilter = "all";
    showAll = false;
    chips.forEach(c => c.classList.remove("active"));
    chips.find(c => c.getAttribute("data-year") === "all").classList.add("active");
    apply();
  });

  apply();
})();
</script>
<!-- ===== End News ===== -->


						
<!-- <h2 style="color: black;">News</h2>

<ul>	  <li> <a style="font-family:sans-serif" style="color:blue;">[Jan 2024]</a>  Promoted as Senior AI Scientist</li>
	      <li> <a style="font-family:sans-serif" style="color:blue;">[Feb. 2023]</a>  Joined Johnson & Johnson as AI Scientist</li>
		  <li> <a style="font-family:sans-serif" style="color:blue;">[Jan. 2023]</a>  Successfully defended my PhD Dissertation Defense.</li>
		  <li> <a style="font-family:sans-serif" style="color:blue;">[May 2022]</a>   One paper got accepted to MIUA 2022.</li>	
		  <li> <a style="font-family:sans-serif" style="color:blue;">[May 2022]</a>   Got accepted to Oxford ML Summer School (~ 5% acceptance rate).</li>
		  <li> <a style="font-family:sans-serif" style="color:blue;">[Mar. 2022]</a>  Successfully Passed my Ph.D. Candidacy Exam.</li>	  
		  <li> <a style="font-family:sans-serif" style="color:blue;">[Oct. 2021]</a>  Attended the <a href="https://researchsummit.microsoft.com/home_public" style="color:blue"> Microsoft Research Summit 2021.</li>	  
	          <li> <a style="font-family:sans-serif" style="color:blue;">[Aug. 2021]</a>  Started Research Internship at <a href="https://www.philips.com/a-w/research/locations/cambridge-north-america.html" style="color:blue"> Philips, </a> Research North America, Cambridge, MA, USA.</li>	  
		  <li> <a style="font-family:sans-serif" style="color:blue;">[May. 2021]</a>  Got accepted to UCL Medical Image Computing Summer School (MedICSS), London</a>.</li>
		  <li> <a style="font-family:sans-serif" style="color:blue;">[Mar. 2021]</a>  Accepted offer as AI Research Intern at <a href="https://www.usa.philips.com/" style="color:blue"> Philips Research</a>, Cambridge, Massachusetts.</li>
	          <li> <a style="font-family:sans-serif" style="color:blue;">[Nov. 2020]</a>  Started Winter School at Center for Computational Medicine in Cardiology, Switzerland</a>.</li>
		  <li> <a style="font-family:sans-serif" style="color:blue;">[Oct. 2020]</a>  Paper got accepted at <a href="https://spie.org/conferences-and-exhibitions/medical-imaging?SSO=1" style="color:blue" > SPIE 2021,</a> San Diego, California.</li>
                  <li> <a style="font-family:sans-serif" style="color:blue;">[Aug. 2020]</a>  Received MICCAI Student Award as a part of NSF grant</a>.</li>
                  <li> <a style="font-family:sans-serif" style="color:blue;">[Aug. 2020]</a>  Started Research Internship at <a href="https://www.research.ibm.com/" style="color:blue"> IBM, </a> Almaden Research Center, San Jose, California.</li>
                  <li> <a style="font-family:sans-serif" style="color:blue;">[May. 2020]</a>  Accepted offer as Research Intern at <a href="https://www.ibm.com/us-en/" style="color:blue"> IBM </a>, San Jose, California.</li>
                  <li> <a style="font-family:sans-serif" style="color:blue;">[Apr. 2020]</a>  Presented paper at ISBI 2020</a>.</li>
                  <li> <a style="font-family:sans-serif" style="color:blue;">[Feb. 2020]</a>  Reviewer for MICCAI 2020.</a>.</li>
                  <li> <a style="font-family:sans-serif" style="color:blue;">[Nov. 2019]</a>  U-NetPlus paper accepted for oral presentation at RIT Graduate Showcase 2019</a>.</li>
<!-- 	<li>
		[12/2019] Paper on unpaired multi-modal learning was accepted by IEEE TMI.
	</li> -->
</ul> -->


<h2 style="color: black;">Professional Experience</h2>
<style>
/* ===== Experience Card ‚Äì Artistic ===== */
.exp-card{
  font-family: Aptos, Arial, sans-serif;
  position: relative;
  padding: 18px 22px 22px 22px;
  border-radius: 18px;
  background: linear-gradient(180deg, #ffffff 0%, #fafafa 100%);
  border: 1px solid #e6e6e6;
  box-shadow: 0 10px 30px rgba(0,0,0,0.05);
  margin-bottom: 30px;
}

/* left accent */
.exp-card:before{
  content:"";
  position:absolute;
  left:0;
  top:0;
  bottom:0;
  width:5px;
  border-radius: 18px 0 0 18px;
  background: linear-gradient(180deg,#d61a1a,#ff7a7a);
}

/* header */
.exp-header{
  display:flex;
  justify-content:space-between;
  align-items:flex-start;
  gap:20px;
}
.exp-meta b{ font-size:20px; }
.exp-meta div{ color:#666; margin-top:2px; }

.exp-logo img{
  width:210px;
  border-radius:12px;
}

/* sections */
.exp-section{
  margin-top:20px;
  padding:14px 16px;
  border-radius:14px;
  background:#ffffff;
  border:1px solid #efefef;
  transition: transform 180ms ease, box-shadow 180ms ease;
}
.exp-section:hover{
  transform: translateY(-2px);
  box-shadow:0 6px 18px rgba(0,0,0,0.06);
}

.exp-title{
  font-weight:800;
  margin-bottom:8px;
  font-size:17px;
}

/* bullets */
.exp-list{
  margin:0;
  padding-left:18px;
}
.exp-list li{
  margin:6px 0;
  line-height:1.55;
}

/* tag system */
.exp-tags{ margin-top:8px; }
.tag{
  display:inline-block;
  padding:4px 12px;
  margin:4px 6px 0 0;
  border-radius:999px;
  font-size:13px;
  border:1px solid;
}

/* domain colors */
.cv   { background:#f2f7ff; border-color:#c9dcff; color:#1f4bb8; }
.agent{ background:#f8f5ff; border-color:#ded0ff; color:#5b2fbf; }
.gen  { background:#fff5f5; border-color:#ffd2d2; color:#b91c1c; }

</style>

<div class="exp-card">

  <!-- Header -->
  <div class="exp-header">
    <div class="exp-meta">
      <b>Johnson & Johnson</b>
      <div>Senior AI Scientist </div>
      <div>New Jersey</div>
      <div>Feb 2023 ‚Äì Present</div>
    </div>

    <div class="exp-logo">
      <img src="./indexpics/Johnson-and-Johnson.jpeg" alt="Johnson & Johnson">
    </div>
  </div>

  <!-- Computer Vision -->
  <div class="exp-section">
    <div class="exp-title">Deep Learning, AI &amp; Computer Vision</div>
    <ul class="exp-list">
      <li>Implemented a large-scale Vision Transformer (ViT) architecture for multi-scale segmentation and detection of Stage IV multi-lesion in volumetric lung CT images, leveraging hierarchical attention to capture long-range spatial dependencies, achieving a
5% accuracy gain over convolutional baselines.</li>
<li>Applied the ViT framework for quantification of key oncology metrics (lesion count, RECIST target lesion, volume, diameter),
enabling interpretable outputs and clinically actionable insights that supported the MONARCH Robotic Bronchoscopy System
for targeted intratumoral therapy planning.</li>
    </ul>
    <div class="exp-tags">
      <span class="tag cv">3D CT</span>
      <span class="tag cv">Segmentation</span>
      <span class="tag cv">SAM / MedSAM</span>
      <span class="tag cv">Vision Transformers</span>
      <span class="tag cv">Self / Semi-Supervised</span>
    </div>
  </div>
	  <img src="./indexpics/multi_lesion.gif" alt="this slowpoke moves"  width="280" alt="404 image"/>
	  <img src="./indexpics/lung_airways.jpeg" width="250" height="160" class="center">
  <!-- Agentic AI -->
  <div class="exp-section">
    <div class="exp-title">Agentic AI: Multi-Agent System</div>
    <ul class="exp-list">
      <li>Built an agentic AI system using CrewAI for autonomous, multi-agent collaboration.</li>
      <li>Agents execute tasks via external APIs with structured communication and role specialization.</li>
      <li>Planning, reasoning, and memory enable adaptive behavior based on context and prior outcomes.</li>
    </ul>
    <div class="exp-tags">
      <span class="tag agent">CrewAI</span>
      <span class="tag agent">Tool Use</span>
      <span class="tag agent">Planning</span>
      <span class="tag agent">Memory</span>
      <span class="tag agent">API Orchestration</span>
    </div>
  </div>

  <!-- GenAI -->
  <div class="exp-section">
    <div class="exp-title">Generative AI: NLP</div>
    <ul class="exp-list">
      <li>Conduct R&amp;D to improve LLM performance for text-classification tasks.</li>
      <li>Design and implement automation workflows using EMR / EHR / claims data.</li>
      <li>Develop GenAI healthcare models leveraging foundation NLP architectures.</li>
      <li>Capabilities include prompt engineering, full fine-tuning, PEFT, and retrieval-augmented workflows.</li>
    </ul>
    <div class="exp-tags">
      <span class="tag gen">Prompt Engineering</span>
      <span class="tag gen">Fine-Tuning</span>
      <span class="tag gen">PEFT</span>
      <span class="tag gen">Vector DB</span>
      <span class="tag gen">HuggingFace</span>
      <span class="tag gen">LangChain</span>
      <span class="tag gen">OpenAI</span>
    </div>
  </div>

</div>

<!-- ===== START Experience ===== -->
<ul>

  <!-- ================= PHILIPS ================= -->
  <img align="right" src="./indexpics/philips.png" width="250px" class="right">

  <li style="padding-bottom:25px; color:black;">
    <b>Philips Research</b><br>

    <span style="color:gray;">AI Research Intern 2021</span><br>
    <span style="color:gray;">Cambridge, Massachusetts</span><br>
    <span style="color:gray;">Aug 2021 ‚Äì Nov 2021</span><br><br>

    <ul class="roman">
        </div>
        <div class="about-sub" style="margin:0 0 14px 0;">
          <b>Real-Time Embedded Object Detection for Portable Imaging Devices</b>
        </div>
      <li>
        Designed an optimized (60 fps) semi-supervised object tracking model for early detection of lung consolidation from ultrasound
		videos and integration into PHILIPS LUMIFY portable ultrasound device, enhancing detection accuracy (mAP) by 38% while,
		reducing model complexity by 86% (from 2.2M to 0.3M) through efficient encoder-decoder design, enabling real-time inference
      </li>
      <br>
      <img src="./indexpics/lumify.webp" width="170px" class="center">
      <img src="./indexpics/con.jpeg" width="170px" class="center">
    </ul>
  </li>

  <!-- ================= IBM ================= -->
  <img align="right" src="./indexpics/ibm.png" width="320px" class="right">

  <li style="padding-bottom:25px; color:black;">
    <b>IBM Research</b><br>

    <span style="color:gray;">Machine Learning Research Intern 2020</span><br>
    <span style="color:gray;">San Jose, California</span><br>
    <span style="color:gray;">Aug 2020 ‚Äì Nov 2020</span><br><br>

    <ul class="roman">
        </div>
        <div class="about-sub" style="margin:0 0 14px 0;">
          <b>Model Compression for Explainable Imaging AI in Classification</b>
        </div>
		
      <li>
        <b>[Project 1]</b> Prototype an explainable AutoML repository of subnetworks
        based on similarity and ranking algorithms achieving
        <b>82.9%</b> fewer parameters and <b>28√ó</b> higher accuracy than baseline
      </li>
      <li>
        <b>[Project 2]</b> Restructured channel-wise pruning of transfer-learning
        convolutional layers, achieving up to <b>99.5%</b> parameter pruning
        and <b>95.4%</b> FLOPs reduction
      </li>
      <li>
        Scripted <b>1,500+</b> lines of code and delivered the project on time
      </li>
      <br>
      <img src="./indexpics/IBM1.png" width="170px" class="center">
      <img src="./indexpics/IBM2.jpeg" width="170px" class="center">
    </ul>
  </li>

  <!-- ================= OXFORD ================= -->
  <img align="right" src="./indexpics/OXF.png" width="90px" class="right">

  <li style="padding-bottom:25px; color:black;">
    <b>University of Oxford</b><br>

    <span style="color:gray;">Machine Learning Summer School 2022</span><br>
    <span style="color:gray;">Oxford, United Kingdom</span><br>
    <span style="color:gray;">Jun 2022 ‚Äì Aug 2022</span><br><br>

    <ul class="roman">
      <li>
        Statistical Learning ‚Ä¢ Markov Decision Theory ‚Ä¢
        Self-supervised Learning (<b>Top 5%</b>)
      </li>
    </ul>
  </li>

</ul>

<!-- ===== End Experience ===== -->


	<img align="right" src="./indexpics/rit.png" width="320px" class="right">
   	<li style="padding-bottom:25px;color: black;">
	<b> Rochester Institute of Technology </b><br>
	<a  style="color:gray"  > Research Assistant </a> <br>
	<a  style="color:gray"  > RIT Biomedical Modeling, Visualization and Image-guided Navigation Lab </a> <br>
	<a  style="color:gray"  > Rochester, NY </a> <br>
	<a  style="color:gray"  > Aug 2018 - Nov 2022 </a> <br>
	 <br>
	Advisor: <a href="https://www.rit.edu/directory/calbme-cristian-linte" style="color:blue"  > Cristian A. Linte, Ph.D</a><br>
		<ul class="roman">
		    <li> Tailored a Student-Teacher (gradient-to-gradient) augmentation-driven meta pseudo-labeling model by distilling knowledge through self-training, scaling up a 4.4% improvement in 3D semantic cardiac segmentation accuracy with a statistical hypothesis testing experiments on only 10% labeled data</li>
		    <li> Extracted noise-free 3D isosurface mesh with smoothing marching cubes algorithm and generated deformation field from a VoxelMorph-based 4D registration framework to predict cardiac motion which was four times faster than the baseline</li>  
		    <li> Acquired solid knowledge of imaging chain, including optics, sensors, ISP, and psycho-physical experiments with multi-view geometry of computer vision ranging from sensor fusion (LIDAR and Camera) to camera calibration (intrinsic and extrinsic parameter estimation, Epipolar geometry/stereo vision), image localization (edge, line, corner, and blob detection), perception, and statistical signal processing</li>
        </ul>
    </li>
	<p style="margin-top:3px"></p>	
	</li>
</ul>


<!-- <h2>Patent</h2>
<ul>
	<li>
		<a href="https://patentscope2.wipo.int/search/en/detail.jsf?docId=WO2017005591">Method and device for detecting pulmonary nodule in computed tomography image, and computer-readable storage medium</a>.<br>
		Qi Dou, <b>Quande Liu</b>, Hao Chen.<br>
		US Patent US20200005460A1, 2018.<br>
	</li>
</ul> -->

<!-- <h2>Selected Honors &amp; Awards</h2>
<table style="border-spacing:2px">
	
		<tbody>
		<tr><td> Microsoft Research Asia (MSRA) PhD Fellowship Nomination Award (2020) </td></tr>
		<tr><td> Outstanding Graduates Award, Zhejiang Province (2018)</td></tr>
		<tr><td> Zhejiang University Scholarship (2015-2017)</td></tr>
		<tr><td> Runner-up in 11th Robot Competition, Zhejiang University, 2017</td></tr>
		<tr><td> Second-class Scholarship for Outstanding Students, 2015-2017</td></tr>
		<tr><td> Second-class Academic Scholarship, 2015-2017</td></tr>
		<tr><td> Admission to Chu Kochen Honors College, Zhejiang University (2014)</td></tr>
	</tbody>
</table> -->

<!-- ===== Software Development (Soft Product Gallery) ===== -->
<style>
  .sw-wrap{
    font-family: Aptos, Arial, sans-serif;
    margin: 24px 0 40px;
  }

  .sw-title{
    font-size: 28px;
    font-weight: 850;
    letter-spacing: -0.3px;
    margin: 0 0 18px;
    color:#111;
    border-bottom: 1px solid #eaeaea;
    padding-bottom: 6px;
  }

  /* --- Product block --- */
  .sw-item{
    display:grid;
    grid-template-columns: 1.2fr 0.8fr;
    gap: 30px;
    margin: 34px 0;
    padding: 26px;
    border-radius: 22px;
    background:
      linear-gradient(180deg, rgba(0,0,0,0.02), rgba(0,0,0,0.00)),
      radial-gradient(1200px 300px at 0% 0%, rgba(0,0,0,0.03), transparent 55%);
  }

  .sw-item.reverse{
    grid-template-columns: 0.8fr 1.2fr;
  }

  /* --- Text --- */
  .sw-name{
    font-size: 22px;
    font-weight: 800;
    margin: 0;
    color:#111;
    letter-spacing: -0.2px;
  }

  .sw-sub{
    margin: 6px 0 14px;
    font-size: 15px;
    color:#666;
  }

  .sw-desc{
    font-size: 15.5px;
    line-height: 1.6;
    color:#111;
    margin-bottom: 14px;
  }

  .sw-link{
    display:inline-block;
    margin-top: 6px;
    font-size: 14px;
    color:#1f4bb8;
    text-decoration:none;
    font-weight: 600;
  }
  .sw-link:hover{ text-decoration: underline; }

  /* --- Visuals --- */
  .sw-visual{
    display:flex;
    align-items:center;
    justify-content:center;
    gap: 16px;
  }

  .sw-card{
    background:#fff;
    border-radius: 22px;
    padding: 14px;
    box-shadow: 0 20px 40px rgba(0,0,0,0.10);
    transition: transform 220ms ease, box-shadow 220ms ease;
  }

  .sw-card:hover{
    transform: translateY(-4px);
    box-shadow: 0 26px 55px rgba(0,0,0,0.14);
  }

  .sw-card img{
    display:block;
    width: 100%;
    height:auto;
    border-radius: 14px;
  }

  /* --- Tag strip --- */
  .sw-tags{
    display:flex;
    flex-wrap:wrap;
    gap: 10px;
    margin-top: 16px;
  }

  .sw-tag{
    font-size: 13px;
    padding: 5px 12px;
    border-radius: 999px;
    background: rgba(0,0,0,0.05);
    color:#333;
  }

  @media (max-width: 900px){
    .sw-item,
    .sw-item.reverse{
      grid-template-columns: 1fr;
    }
  }
</style>

<div class="sw-wrap">
  <div class="sw-title">Software Development</div>

  <!-- ===== NIFTI VIEWER ===== -->
  <section class="sw-item">
    <div>
      <h3 class="sw-name">NiFTI File Viewer</h3>
      <div class="sw-sub">macOS Compatible ‚Ä¢ Neuroimaging Utility</div>

      <p class="sw-desc">
        Designed and built a native macOS application to visualize
        <b>NiFTI neuroimaging files</b>, enabling intuitive exploration of
        volumetric medical data across coronal, sagittal, and axial planes.
      </p>

      <p class="sw-desc">
        Focused on performance, usability, and clean rendering for research
        workflows in medical imaging.
      </p>

      <a class="sw-link" href="https://github.com/SMKamrulHasan/smkamrulhasan.github.io/tree/main/data/">Download Software ‚Üí</a>

      <div class="sw-tags">
        <span class="sw-tag">macOS</span>
        <span class="sw-tag">Medical Imaging</span>
        <span class="sw-tag">NiFTI</span>
        <span class="sw-tag">3D Visualization</span>
      </div>
    </div>

    <div class="sw-visual">
      <div class="sw-card" style="max-width:160px;">
        <img src="./indexpics/NIFTI-VIEWER-LOGO.png" alt="NiFTI Viewer logo">
      </div>
      <div class="sw-card" style="max-width:260px;">
        <img src="./indexpics/mobile.png" alt="NiFTI Viewer UI">
      </div>
    </div>
  </section>

  <!-- ===== GENAI SUMMARIZER ===== -->
  <section class="sw-item reverse">
    <div class="sw-visual">
      <div class="sw-card" style="max-width:360px;">
        <img src="./indexpics/SUMMARIZER.jpeg" alt="GenAI Summarizer UI">
      </div>
    </div>

    <div>
      <h3 class="sw-name">GenAI Summarizer ChatBot</h3>
      <div class="sw-sub">LLM ‚Ä¢ LangChain ‚Ä¢ Vector Search</div>

      <p class="sw-desc">
        Built a <b>GenAI-powered summarization chatbot</b> capable of extracting
        concise, high-quality summaries from PDFs, DOCX, TXT files,
        and Wikipedia articles.
      </p>

      <p class="sw-desc">
        Uses embedding-based retrieval and context-aware prompting
        to provide accurate, source-grounded responses.
      </p>

      <a class="sw-link" href="https://github.com/SMKamrulHasan/smkamrulhasan.github.io/tree/main/data/GenAI_Summarizer_ChatBot.py">Download Software ‚Üí</a>

      <div class="sw-tags">
        <span class="sw-tag">LangChain</span>
        <span class="sw-tag">ChromaDB</span>
        <span class="sw-tag">LLM</span>
        <span class="sw-tag">RAG</span>
        <span class="sw-tag">Embeddings</span>
      </div>
    </div>
  </section>
</div>
<!-- ===== End Software Development ===== -->
				 
<!-- <h2 style="color: black;">Software Development</h2>
<ul>
    <img align="right" src="./indexpics/mobile.png" width="350px" class="right">
    <li style="padding-bottom:25px;color: black;">
        <b> NifTi File Viewer </b><br>
        <a style="color:gray"> macOS Compatible</a> <br>
        <br>
        <ul class="roman">
            <li>Built macOS software to view NIfTI files which is a type of file format for neuroimaging</li>
            <p>[<a href="https://github.com/SMKamrulHasan/smkamrulhasan.github.io/tree/main/data" style="color:blue"> Download Software </a>]</p>
            <br>
            <img src="./indexpics/NIFTI-VIEWER-LOGO.png" width="150px" class="center">
        </ul>
    </li>
    <li style="padding-bottom:25px;color: black;">
        <b> GenAI Summarizer ChatBot </b><br>
        <a style="color:gray"> LLM: Langchain, Chroma Vector Database, Embedding Model</a> <br>
        <br>
        <ul class="roman">
            <li>Built GenAI powered content Summarizer ChatBot that summarizes Pdf, Docx, txt as well as Wikipedia</li>
			<p>[<a href="https://github.com/SMKamrulHasan/smkamrulhasan.github.io/tree/main/data/GenAI_Summarizer_ChatBot.py" style="color:blue"> Download Software </a>]</p>        
            <br>
            <img align="left" src="./indexpics/SUMMARIZER.jpeg" width="350px" class="left">
        </ul>
        <br clear="both">
    </li>
</ul> -->

<h2 style="color: black;">Publications</h2>
<table id="tbPublications" width="100%">
	<tbody>

	* indicates equal contribution;	
								 
	<tr>	
		<td width="270">
		<img src="./indexpics/inpaint.png" width="250px" class="center">
		</td style="text-align:justify">		    
	    
		<td> <font color="blue"> Inpainting Surgical Occlusion from Laparoscopic Video Sequences for Robot-assisted Interventions </font>  <br>
		<em> Journal of Medical Imaging </em>, 2023
		<strong></strong>
		<p>[<a href="https://link.springer.com/chapter/10.1007/978-3-031-12053-4_28" style="color:blue" >paper</a>][<a href="https://github.com/SMKamrulHasan/MTCTL" style="color:blue"  >code</a>][<a href="https://acdc.creatis.insa-lyon.fr/description/" style="color:blue" >dataset</a>][<a href="papers/2020/iteravg/ref.bib" style="color:blue" >bibtex</a>][<a href="https://youtu.be/shC0fjehT0k" style="color:blue" >Video 1</a>] </p>
		<p style="padding-bottom:30px;text-align:justify">
		With the aid of digital inpainting algorithms, this paper presents a novel application that uses image segmentation to remove surgical instruments from laparoscopic/endoscopic video. 
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
								 
								 
								 
	<tr>	
		<td width="270">
		<img src="./indexpics/cqsl.png" width="250px" class="center">
		</td style="text-align:justify">		    
	    
		<td> <font color="blue"> Learning Deep Representations of Cardiac Structures for 4D Cine MRI Image Segmentation through Semi-supervised Learning </font>  <br>
		<em> Applied Science </em>, 2022
		<strong></strong>
		<p>[<a href="https://link.springer.com/chapter/10.1007/978-3-031-12053-4_28" style="color:blue" >paper</a>][<a href="https://github.com/SMKamrulHasan/MTCTL" style="color:blue"  >code</a>][<a href="https://acdc.creatis.insa-lyon.fr/description/" style="color:blue" >dataset</a>][<a href="papers/2020/iteravg/ref.bib" style="color:blue" >bibtex</a>][<a href="https://youtu.be/shC0fjehT0k" style="color:blue" >Video 1</a>] </p>
		<p style="padding-bottom:30px;text-align:justify">
		In this paper, we propose a semi-supervised model --- namely, Combine-all in Semi-Supervised Learning (CqSL) --- to demonstrate the power of a simple combination of a disentanglement block, variational autoencoder (VAE), generative adversarial network (GAN), and a conditioning layer-based reconstructor for performing two important tasks in medical imaging: segmentation and reconstruction. Our work is motivated by the recent progress in image segmentation using semi-supervised learning (SSL), which has shown good results with limited labeled data and large amounts of unlabeled data.
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
		
		
	<tr>	
		<td width="270">
		<img src="./indexpics/organMNIST.png" width="250px" class="center">
		</td style="text-align:justify">		    
	    
		<td> <font color="blue"> The impact of class-dependent label noise in medical image (MedMNIST dataset) classification </font>  <br>
		<em> SPIE Medical Imaging -- Image Processing </em>, 2023
		<strong></strong>
		<p>[<a href="https://link.springer.com/chapter/10.1007/978-3-031-12053-4_28" style="color:blue" >paper</a>][<a href="https://github.com/SMKamrulHasan/MTCTL" style="color:blue"  >code</a>][<a href="https://acdc.creatis.insa-lyon.fr/description/" style="color:blue" >dataset</a>][<a href="papers/2020/iteravg/ref.bib" style="color:blue" >bibtex</a>][<a href="https://youtu.be/shC0fjehT0k" style="color:blue" >Video 1</a>] </p>
		<p style="padding-bottom:30px;text-align:justify">
		In this paper, we study this hypothesis using two publicly available datasets: a 2D organ classification dataset with target organ classes being visually distinct, and a histopathology image classification dataset where the target classes look very similar visually. Our results show that the label noise in one class has much higher impact on the model's performance on other classes for histopathology dataset compared to the organ dataset.
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
		
		
	<tr>	
		<td width="270">
		<img src="./indexpics/miua.gif" alt="this slowpoke moves"  width="250" alt="404 image"/>
		</td style="text-align:justify">		    
			    	    
			    
		<td> <font color="blue"> STAMP: A Self-training Student-Teacher Augmentation-driven Meta Pseudo-labeling Framework for 3D Cardiac MRI Image Segmentation </font>  <br>
		<strong>S. M. Kamrul Hasan</strong> and Cristian A. Linte. <br>
		<em> Medical Image Understanding and Analysis (MIUA) </em>, 2022
		<strong>oral</strong>
		<p>[<a href="https://link.springer.com/chapter/10.1007/978-3-031-12053-4_28" style="color:blue" >paper</a>][<a href="https://github.com/SMKamrulHasan/MTCTL" style="color:blue"  >code</a>][<a href="https://acdc.creatis.insa-lyon.fr/description/" style="color:blue" >dataset</a>][<a href="papers/2020/iteravg/ref.bib" style="color:blue" >bibtex</a>][<a href="https://youtu.be/shC0fjehT0k" style="color:blue" >Video 1</a>] </p>
		<p style="padding-bottom:30px;text-align:justify">
		The proposed method uses self-training (through meta pseudo-labeling) in concert with a Teacher network that instructs the Student network by generating pseudo-labels given unlabeled input data. Meta pseudo-labeling methods allow the Teacher network to constantly adapt in response to the performance of the Student network on the labeled dataset, hence enabling the Teacher to identify more effective pseudo-labels to instruct the Student. Moreover, to improve generalization and reduce error rate, we apply both strong and weak data augmentation policies, to ensure the segmentor outputs a consistent probability distribution regardless of the augmentation level.
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
	
		
	<tr>	
		<td width="270">
		<img src="./indexpics/embc.gif" alt="this slowpoke moves"  width="250" alt="404 image"/>
		<img src="./indexpics/embc22_2.png" width="250px" class="center">
		</td style="text-align:justify">		    
			    
		<td> <font color="blue"> Joint Segmentation and Uncertainty Estimation of Ventricular Structures from Cardiac MRI using Probability Calibration </font>  <br>
		<strong>S. M. Kamrul Hasan</strong> and Cristian A. Linte. <br>
		<em> International Conference of the Eng. in Med. & Bio (EMBC)</em>, 2022
		<strong>oral</strong>
		<p>[<a href="https://www.researchgate.net/publication/346026748_Segmentation_and_Removal_of_Surgical_Instruments_for_Background_Scene_Visualization_from_Endoscopic_Laparoscopic_Video" style="color:blue" >paper</a>][<a href="https://github.com/SMKamrulHasan/MTCTL" style="color:blue"  >code</a>][<a href="https://acdc.creatis.insa-lyon.fr/description/" style="color:blue" >dataset</a>][<a href="papers/2020/iteravg/ref.bib" style="color:blue" >bibtex</a>][<a href="https://youtu.be/shC0fjehT0k" style="color:blue" >Video 1</a>] </p>
		<p style="padding-bottom:30px;text-align:justify">
		In this work, we used a Bayesian version of our previously proposed CondenseUNet framework featuring both a learned group structure and a regularized weight-pruner to reduce the computational cost in volumetric image segmentation and help quantify predictive uncertainty. Our study further showcases the potential of our deep-learning framework to evaluate the correlation between the uncertainty and the segmentation errors for a given model. The proposed model was trained and tested on the Automated Cardiac Diagnosis Challenge (ACDC) dataset featuring 150 cine cardiac MRI patient dataset for the segmentation and uncertainty estimation of the left ventricle (LV), right ventricle (RV), and myocardium (Myo) at end-diastole (ED) and end-systole (ES) phases.
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
		
		
	<tr>	
		<td width="270">
		<img src="./indexpics/spie22.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		<img src="./indexpics/spie22_2.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		</td style="text-align:justify">		    
			    
		<td> <font color="blue">Calibration of cine MRI segmentation probability for uncertainty estimation using a Multi-Task Cross-Task Learning architecture.</font>  <br>
		<strong>S. M. Kamrul Hasan</strong> and Cristian A. Linte. <br>
		<em>SPIE Medical Imaging</em>, 2022
		<strong>oral</strong>
		<p>[<a href="https://spie.org/medical-imaging/presentation/Calibration-of-cine-MRI-segmentation-probability-for-uncertainty-estimation-using/12034-21" style="color:blue" >paper</a>][<a href="https://github.com/SMKamrulHasan/MTCTL" style="color:blue"  >code</a>][<a href="https://acdc.creatis.insa-lyon.fr/description/" style="color:blue" >dataset</a>][<a href="papers/2020/iteravg/ref.bib" style="color:blue" >bibtex</a>][<a href="https://youtu.be/shC0fjehT0k" style="color:blue" >Video 1</a>] </p>
		<p style="padding-bottom:30px;text-align:justify">
		In this work we propose a novel method that incorporates uncertainty estimation to detect failures in the segmentation masks generated by CNNs, our study further showcases the potential of our model to evaluate the correlation between the uncertainty and the segmentation errors for a given model. Furthermore, we introduce a multi-task Cross-task learning consistency approach to enforce the correlation between the pixel-level (segmentation) and the geometric-level (distance map) tasks. Our study serves as a proof-of-concept of how uncertainty measure correlates with the erroneous segmentation generated by different deep learning models, further showcasing the potential of our model to flag low-quality segmentation from a given model in our future study.
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
		
		
	<tr>	
		<td width="270">
		<img src="./indexpics/embc21.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		<img src="./indexpics/embc21_2.PNG" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		</td style="text-align:justify">		    
			    
		<td> <font color="blue">Motion Extraction of the Right Ventricle from 4D Cardiac Cine MRI Using A Deep Learning-Based Deformable Registration Framework.</font>  <br>
		Roshan Reddy Upendra*, <strong>S. M. Kamrul Hasan</strong>*, Richard Simon, Brian Jamison Wentz, Suzanne M. Shontz, Michael S. Sacks, and Cristian A. Linte. <br>
		"The first two authors share equal joint first authorship"<br>
		<em> International Conference of the Engineering in Medicine & Biology Society (EMBC)</em>, 2021,
		<strong>oral</strong>
		<p>[<a href="https://pubmed.ncbi.nlm.nih.gov/34892062/" style="color:blue" >paper</a>][<a href="https://github.com/SMKamrulHasan/RV_deformation" style="color:blue"  >code</a>][<a href="https://acdc.creatis.insa-lyon.fr/description/" style="color:blue" >dataset</a>][<a href="papers/2020/iteravg/ref.bib" style="color:blue" >bibtex</a>][<a href="https://youtu.be/shC0fjehT0k" style="color:blue" >Video 1</a>] </p>
		<p style="padding-bottom:30px;text-align:justify">
		In this work, we describe the development of dynamic patient-specific right ventricle (RV) models associated with normal subjects and abnormal RV patients to be subsequently used to assess RV function based on motion and kinematic analysis. In our study, we use a deep learning-based deformable network that takes 3D input volumes and outputs a motion field which is then used to generate isosurface meshes of the cardiac geometry at all cardiac frames by propagating the end-diastole (ED) isosurface mesh using the reconstructed motion field.
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
		
	<tr>	
		<td width="270">
		<img src="./images/cinc0.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		<img src="./images/cinc1.PNG" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		<img src="./images/cinc2.gif" alt="this slowpoke moves"  width="250" alt="404 image"/>
		</td style="text-align:justify">		    
			    
		<td> <font color="blue">A Multi-Task Cross-Task Learning Architecture for Ad-hoc Uncertainty Estimation in 3D Cardiac MRI Image Segmentation.</font>  <br>
		<strong>S. M. Kamrul Hasan</strong> and Cristian A. Linte. <br>
		<em>Computing in Cardiology </em>, 2021,
		<strong>oral</strong>
		<p>[<a href="https://ieeexplore.ieee.org/document/9662869" style="color:blue" >paper</a>][<a href="https://github.com/SMKamrulHasan/MTCTL" style="color:blue"  >code</a>][<a href="http://atriaseg2018.cardiacatlas.org/" style="color:blue" >dataset</a>][<a href="papers/2020/iteravg/ref.bib" style="color:blue" >bibtex</a>][<a href="https://youtu.be/shC0fjehT0k" style="color:blue" >Video 1</a>] </p>
		<p style="padding-bottom:30px;text-align:justify">
		To generate smoother and accurate segmentation masks from 3D cardiac MR images, we present a Multi-task Cross-task learning consistency approach to enforce the correlation between the pixel-level (segmentation) and the geometric-level (distance map) tasks. Our extensive experimentation with varied quantities of labeled data in the training sets justify the effectiveness of our model for the segmentation of left atrial cavity from Gadolinium-enhanced magnetic resonance (GE-MR) images. With the incorporation of uncertainty estimates to detect failures in the segmentation masks generated by CNNs, our study further showcases the potential of our model to flag low quality segmentation from a given model.
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
		
		
	<tr>	
		<td width="270">
		<img src="./indexpics/hasan8gf.gif" alt="this slowpoke moves"  width="250" alt="404 image"/>
		<img src="./indexpics/kamrul2.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		<img src="./indexpics/hasan6gf.gif" alt="this slowpoke moves"  width="250" alt="405 image"/>
		</td style="text-align:justify">		    
			    
		<td> <font color="blue">Segmentation and removal of surgical instruments for background scene visualization from Endoscopic / Laparoscopic video.</font>  <br>
		<strong>S. M. Kamrul Hasan</strong>, Richard A. Simon, and Cristian A. Linte. <br>
		<em>SPIE Medical Imaging</em>, 2021,
		<strong>oral</strong>
		<p>[<a href="https://www.researchgate.net/publication/346026748_Segmentation_and_Removal_of_Surgical_Instruments_for_Background_Scene_Visualization_from_Endoscopic_Laparoscopic_Video" style="color:blue" >paper</a>][<a href="https://github.com/SMKamrulHasan/Video_inpainting" style="color:blue"  >code</a>][<a href="https://endovis.grand-challenge.org/" style="color:blue" >dataset</a>][<a href="papers/2020/iteravg/ref.bib" style="color:blue" >bibtex</a>][<a href="https://youtu.be/shC0fjehT0k" style="color:blue" >Video 1</a>] [<a href="https://youtu.be/68kBjT60hkw" style="color:blue" >Video 2 </a>][<a href="https://youtu.be/--DnWQKsUHQ" style="color:blue" >Video 3</a>] [<a href="https://youtu.be/tIrRnB0K1b0" style="color:blue" >Video 4</a>] </p>
		<p style="padding-bottom:30px;text-align:justify">
		In this work, we implement a fully convolutional segmenter featuring both a learned group structure and a regularized weight-pruner to reduce the high computational cost in volumetric image segmentation. We validated our framework on the ACDC dataset featuring one healthy and four pathology groups imaged throughout the cardiac cycle. Based on these results, this technique has the potential to become an efficient and competitive cardiac image segmentation tool that may be used for cardiac computer-aided diagnosis, planning and guidance applications.
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
		
	<tr>
		<td width="270">
		<img src="./indexpics/RESULT2.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		<img src="./indexpics/PLOT1.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		</td style="text-align:justify">		
		<td> <font color="blue">L-CO-Net: Learned Condensation-Optimization Network for Clinical Parameter Estimation from Cardiac Cine MRI.</font>  <br>
		<b><strong>S. M. Kamrul Hasan</strong></b>, and Cristian A. Linte. <br>
        <em> International Conference of the Engineering in Medicine & Biology Society (EMBC)</em>, 2020,
        <strong>oral</strong>
		<p>[<a href="https://ieeexplore.ieee.org/document/9176491" style="color:blue"  >paper</a>][<a href="https://github.com/SMKamrulHasan/Regularized-Network" style="color:blue" >code</a>][<a href="https://www.creatis.insa-lyon.fr/Challenge/acdc/databases.html" style="color:blue" >dataset</a>][<a href="papers/2020/iteravg/ref.bib" style="color:blue" >bibtex</a>]</p>
        <p style="padding-bottom:30px;text-align:justify">
          In this work, we implement a fully convolutional segmenter featuring both a learned group structure and a regularized weight-pruner to reduce the high computational cost in volumetric image segmentation. We validated our framework on the ACDC dataset featuring one healthy and four pathology groups imaged throughout the cardiac cycle. Our technique achieved Dice scores of 96.8% (LV blood-pool), 93.3% (RV blood-pool) and 90.0% (LV Myocardium) with five-fold cross-validation and yielded similar clinical parameters as those estimated from the ground truth segmentation data. Based on these results, this technique has the potential to become an efficient and competitive cardiac image segmentation tool that may be used for cardiac computer-aided diagnosis, planning, and guidance applications.
        </p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
		
		
	<tr>
		<td width="270">
		<img src="./indexpics/model1.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		<img src="./indexpics/RESULT3.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		<img src="./indexpics/heart_main.gif" alt="this slowpoke moves"  width="250" alt="405 image"/>
		</td style="text-align:justify">		
		<td> <font color="blue">Learned Condensation-Optimization Network: A regularized Network for improved Cardiac Ventricles Segmentation on Breath-Hold Cine MRI.</font>  <br>
		<b><strong>S. M. Kamrul Hasan</strong></b>, and Cristian A. Linte. <br>
        <em> International Symposium on Biomedical Imaging (ISBI)</em>, 2020,
        <strong>oral</strong>
		<p>[<a href="https://www.researchgate.net/publication/340595489_Learned_Condensation-Optimization_Network_A_regularized_Network_for_improved_Cardiac_Ventricles_Segmentation_on_Breath-Hold_Cine_MRI" style="color:blue"  >paper</a>][<a href="https://github.com/SMKamrulHasan/Regularized-Network" style="color:blue" >code</a>][<a href="https://www.creatis.insa-lyon.fr/Challenge/acdc/databases.html" style="color:blue" >dataset</a>][<a href="papers/2020/iteravg/ref.bib" style="color:blue" >bibtex</a>]</p>
        <p style="padding-bottom:30px;text-align:justify">
         In this work, we implement a fully convolutional segmenter featuring both a learned group structure and a regularized weight-pruner to reduce the high computational cost in volumetric image segmentation. We validated the framework on the ACDC dataset and achieved accurate segmentation, leading to mean Dice scores of 96.80% (LV blood-pool), 93.33% (RV blood-pool), 90.0% (LV Myocardium) and yielded similar clinical parameters as those estimated from the ground-truth segmentation data.
        </p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
		
<!-- 	<tr>
		<td width="270">
		<img src="./indexpics/tmi20_cxr.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		</td>		
		<td> Deep Mining External Imperfect Data for Chest X-ray Disease Screening. <br>
		Luyang Luo, Lequan Yu, Hao Chen, <b>Quande Liu</b>, Xi Wang, Jiaqi Xu, Pheng-Ann Heng. <br>
		<em>IEEE Transactions on Medical Imaging (TMI)</em>, 2020.
		<p>[<a href="https://arxiv.org/pdf/2006.03796.pdf">paper</a>]</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
		 -->
	<tr>
		<td width="270">
		<img src="./indexpics/5.png" width="250px" style="box-shadow: 4px 4px 8px #888" >
		<img src="./indexpics/SPIE_2020_result.png" width="250px" style="box-shadow: 4px 4px 8px #888" >
		</td style="text-align:justify">		
		<td> <font color="blue">CondenseUNet: a memory-efficient condensely-connected architecture for bi-ventricular blood pool and myocardium segmentation.</font>  <br>
		<b><strong>S. M. Kamrul Hasan</strong></b>, and Cristian A. Linte. <br>
       		<em>SPIE Medical Imaging</em>, 2020,
		<strong>oral</strong>
		<p>[<a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11315/113151J/CondenseUNet---a-memory-efficient-condensely-connected-architecture-for/10.1117/12.2550640.short?SSO=1" style="color:blue"  >paper</a>][<a href="https://github.com/SMKamrulHasan/CondenseUNet" style="color:blue" >code</a>][<a href="https://www.creatis.insa-lyon.fr/Challenge/acdc/databases.html" style="color:blue" >dataset</a>][<a href="papers/2020/iteravg/ref.bib" style="color:blue" >bibtex</a>]</p>
        <p style="padding-bottom:30px;text-align:justify">
        In this work, we propose a novel memory-efficient Convolutional Neural Network (CNN) architecture as a modification of both CondenseNet, as well as DenseNet for ventricular blood-pool segmentation by introducing a bottleneck block and an upsampling path. Our experiments show that the proposed architecture runs on the Automated Cardiac Diagnosis Challenge (ACDC) dataset using half (50%) the memory requirement of DenseNet and one-twelfth (‚àº 8%) of the memory requirements of U-Net, while still maintaining excellent accuracy of cardiac segmentation.
        </p>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
		
	<tr>
		<td width="270">
		<img src="./indexpics/cinc.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		</td style="text-align:justify">		
		<td> <font color="blue">Toward Quantification and Visualization of Active Stress Waves for Myocardial Biomechanical Function Assessment.</font>  <br>
        Niels F Otani, Dylan Dang, Christopher Beam, Fariba Mohammadi, Brian Wentz, <strong>S. M. Kamrul Hasan</strong>, Suzanne M Shontz, Karl Q Schwarz, Sabu Thomas, and Cristian A. Linte. <br>
		<em>Computing in Cardiology (CinC)</em>, 2019.
		<p>[<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7373340/" style="color:blue" >paper</a>][<a href="https://github.com/SMKamrulHasan" style="color:blue" >code</a>][<a href="" style="color:blue"  >dataset</a>][<a href="papers/2020/iteravg/ref.bib" style="color:blue" >bibtex</a>]</p>
        <p style="padding-bottom:30px;text-align:justify">
         In the forward model, tissue deformation was generated using a test wave with active stresses that mimic the myocardial contractile forces. The generated deformation field was used as input to an inverse model designed to reconstruct the original active stress distribution. We numerically simulated malfunctioning tissue regions (experiencing limited contractility and hence active stress) within the healthy tissue. We also assessed model sensitivity by adding noise to the deformation field generated using the forward model. The difference image between the original and reconstructed active stress distribution suggests that the model accurately estimates active stress from tissue deformation data with a high signal-to-noise ratio.
        </p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
		
	<tr>
		<td width="270">
		<img src="./indexpics/EMBC_2019_result.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		<img src="./indexpics/EMBC_2019_result2.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		<img src="./indexpics/EMBC_2019_result3.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		<img src="./indexpics/parts.gif" alt="this slowpoke moves"  width="250" alt="404 image"/>
		</td style="text-align:justify">		
		<td> <font color="blue">U-NetPlus: A Modified Encoder-Decoder U-Net Architecture for Semantic and Instance Segmentation of Surgical Instruments from Laparoscopic Images.</font>  <br>
		<strong>S. M. Kamrul Hasan</strong>, and Cristian A. Linte. <br>
		<em>International Conference of the IEEE Engineering in Medicine and Biology (EMBC)</em>, 2020,
        <strong>oral</strong>
		<p>[<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7372295/" style="color:blue" >paper</a>][<a href="unetplus.github.io" style="color:blue" >code</a>][<a href="https://endovis.grand-challenge.org/" style="color:blue" >dataset</a>][<a href="papers/2020/iteravg/ref.bib" style="color:blue" >bibtex</a>]</p>
        <p style="padding-bottom:30px;text-align:justify">
         In this work, we modify the U-Net architecture by introducing a pre-trained encoder and re-design the decoder part, by replacing the transposed convolution operation with an upsampling operation based on nearest-neighbor (NN) interpolation. To further improve performance, we also employ a very fast and flexible data augmentation technique. We trained the framework on 8 x 225 frame sequences of robotic surgical videos available through the MICCAI 2017 EndoVis Challenge dataset and tested it on 8 x 75 frame and 2 x 300 frame videos. Using our U-NetPlus architecture, we report a 90.20\% DICE for binary segmentation, 76.26% DICE for instrument part segmentation, and 46.07% for instrument type (i.e., all instruments) segmentation, outperforming the results of previous techniques implemented and tested on these data.
        </p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>
		<td width="270">
		<img src="./indexpics/WNYISPW_2018_model.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		<img src="./indexpics/WNYISPW_2018_nnret.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
			
			
		</td style="text-align:justify">		
		<td> <font color="blue">A Modified U-Net Convolutional Network Featuring a Nearest-neighbor Re-sampling-based Elastic-Transformation for Brain Tissue Characterization and Segmentation.</font>  <br>

        <strong>S. M. Kamrul Hasan</strong>, and Cristian A. Linte. <br>
		<em> Western New York Image and Signal Processing Workshop (WNYISPW)</em>, 2018,
        <strong>oral</strong>
		<p>[<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6583803/" style="color:blue" >paper</a>][<a href="https://github.com/SMKamrulHasan/" style="color:blue" >code</a>][<a href="https://www.med.upenn.edu/sbia/brats2017/data.html" style="color:blue" >dataset</a>][<a href="papers/2020/iteravg/ref.bib" style="color:blue" >bibtex</a>]</p>
        <p style="padding-bottom:30px;text-align:justify">
         Though this model works better on BRATS 2015 dataset by using pixel-wise segmentation map of the input image like an auto-encoder which assures best segmentation accuracy, but it is not correct for all the cases. So, I have planned to improve this U-net model by replacing the de-convolution part with the upsampled by Nearest-neighbor algorithm and also by using elastic transformation for increasing the training dataset to make the model more robust on Low graded tumor. I had trained my NNRET U-net model on BRATS 2017 dataset and got a better performance than the state of the art classic U-net model.
        </p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>


</tbody></table>

<h2 style="color: black;">Talks</h2>
<table id="tbTeaching" border="0" width="100%">
	<tbody>
		<tr>
			<td> 2022</td><td>Podcast</td><td>Bangladeshi Researchers in Data Science and Machine Learning</td>
		</tr>
		<tr>
			<td> 2021</td><td>Presentation</td><td>Philips Research North America</td>
		</tr>
		<tr>
			<td> 2021</td><td>Workshop</td><td>RIT Co-op Placement (Guest Speaker)</td>
		</tr>		
		<tr>
			<td> 2020</td><td>Presentation</td><td>IBM Almaden Research Center</td>
		</tr>
	</tbody>
</table>
	
<h2 style="color: black;">Honors &amp; Awards</h2>
<ul>
	<li>
		<tr><td> MICCAI student travel award as a part of NSF Grant (2020) </td></tr>
	</li>
	<li>
		<tr><td> <a href="https://ewh.ieee.org/r1/rochester/sp/WNYISPW2018.html" style="color:blue" > Best paper award </a>, Western New York Image and Signal Processing Workshop (2018)</td></tr>
	</li>
	<li>
		<tr><td> Imagine Festival RIT Award from KODAK (2017)</td></tr>
	</li>
	<li>
		<tr><td>RIT Graduate Scholarship (2017)</td></tr>
	</li>
	<li>
		<tr><td>Awarded for achieving GPAs of 3.85‚àº4.0 in total of six out of eight semesters (2012-2015)</td></tr>
	</li>
		<!-- <tr><td> Runner-up in 11th Robot Competition, Zhejiang University, 2017</td></tr> -->
		<!-- <tr><td> Second-class Scholarship for Outstanding Students, 2015-2017</td></tr> -->
		<!-- <tr><td> Second-class Academic Scholarship, 2015-2017</td></tr> -->
</ul>

<h2 style="color: black;">Reviewing</h2>
<ul>
	<li style="padding-bottom:30px;" >	
	<b>Conference and Journal Reviews:</b><br>
	Scientific Report (Nature) 2022 <br>
	NeurIPS 2020 <br>
	MICCAI 2020<br>
	IEEE Access 2019<br>
	IJCARS 2020<br>
	IPCAI 2020<br>
	<!-- IEEE Winter Conference on Applications of Computer Vision (WACV) 2020 <br> -->
	</li>	
	<p style="margin-top:3px"></p>		
</ul>

	
<!-- <h2>Teaching</h2>
<table id="tbTeaching" border="0" width="100%">
	<tbody>
		<tr>
			<td> 2019-2020</td><td>Spring</td><td>Principles of Programming Languages (CSCI 3180)</td>
		</tr>
		<tr>
			<td> 2019-2020</td><td>Fall</td><td>Problem Solving by Programming (ENGG 1110)</td>
		</tr>
		<tr>
			<td> 2018-2019</td><td>Spring</td><td>Problem Solving by Programming (ENGG 1110)</td>
		</tr>
		<tr>
			<td> 2018-2019</td><td>Fall</td><td>Digital Logic and Systems (ENGG 2020)</td>
		</tr>
	</tbody>
</table> -->

<!--

<h2>Experience</h2>
<li>
	Research Assistant, &nbsp 08. 2015 - Now
	<p></p>
	<p>&nbsp&nbsp&nbsp The Chinese University of Hong Kong</p>
	<p></p> 
		<p>&nbsp&nbsp&nbsp Advisor: Pheng Ann Heng</p> 
        <p style="margin-top:3px">
		</p>
</li>
<li>
	Software Engineering Intern, &nbsp 01. 2015 - 04. 2015
	<p></p>
	<p>&nbsp&nbsp&nbsp Epiclouds, a startup company in Hangzhou</p>
	<p></p> 
        <p style="margin-top:3px">
		</p>
</li>

	-->


<div id="footer">
	<div id="footer-text"></div>
</div>
	<p><center>
	<td width="270">
	<img src="./indexpics/scanme.JPG" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
	</td style="text-align:justify">
	</div> 
	<!--	
      	<div id="clustrmaps-widget" style="width:40%">
	500	<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=LJNWkxUAFjdgZdHhjWvEOF1K9cIg45om0jzghCyXpkc&cl=ffffff&w=a"></script>
	</div>  
	-->
	<br><center>
        &copy; S. M. Kamrul Hasan | Last updated: June 2022
     
      </center></p>


</div>

</body></html>
