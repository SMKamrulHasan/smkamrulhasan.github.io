<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link rel="shortcut icon" href="myIcon.ico">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />

<meta name="keywords" content="S. M. Kamrul Hasan, S. M. Kamrul Hasan, Center for Imaging Science, RIT"> 
<meta name="description" content="S. M. Kamrul Hasan&#39;s home page">
<meta name="google-site-verification" content="X2QFrl-bPeg9AdlMt4VKT9v6MJUSTCf-SrY3CvKt4Zs" />
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title>S. M. Kamrul Hasan&#39;s Homepage</title>
<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-159069803-1', 'auto');
ga('send', 'pageview');
</script>
<!-- End Google Analytics -->
<!--
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-87320911-1', 'auto');
  ga('send', 'pageview');

</script>
-->
</head>
<body>
<div id="layout-content" style="margin-top:25px">
 <a href="https://github.com/SMKamrulHasan" class="github-corner"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#FD6C6C; color:#fff; position: absolute; top: 0; border: 0; right: 0;"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

<table>
	<tbody>
		<tr>
			<td width="650">
				<div id="toptitle">	 				
					<h1 style="color: black">S. M. Kamrul Hasan, PhD </h1><h1>
				
				<h3 style="color: black"></h3>
				<p style="./data/font-awesome.min.css;padding-bottom:30px" >
					Center for Imaging Science<br>
					Rochester Institute of Technology <br>
					Rochester, New York, USA<br>
					<br>
					Email: sh3190@rit.edu<br>
					Previous:  <a target="_blank" href="https://www.philips.com/a-w/about/innovation/innovation-hubs/cambridge.html">Philips Research</a>; <a target="_blank" href="http://www.research.ibm.com/" target="_blank">IBM Research</a> <br>

		    			[<a target="_blank" href="data/SMKamrul_Hasan_Resume_singlePage.pdf"> CV </a>] [<a target="_blank" href="https://scholar.google.com/citations?user=M7XmUK0AAAAJ&hl=en">Google Scholar</a>]
					
	
				</p>
				<p> <!--<a href="https://scholar.google.com/citations?user=bRe3FlcAAAAJ&hl=en"><img src="./pic/google_scholar.png" height="30px" style="margin-bottom:-3px"></a>--> 
					<a href="https://scholar.google.com/citations?user=M7XmUK0AAAAJ&hl=en#"><img src="./images/gs-logo.png" height="30px" style="margin-bottom:-3px"></a>
					<a href="https://github.com/SMKamrulHasan"><img src="./pic/github_s.jpg" height="30px" style="margin-bottom:-3px"></a>
					<a href="https://www.researchgate.net/profile/S_M_Kamrul_Hasan7"><img src="./pic/rg.png" height="30px" style="margin-bottom:-3px"></a>
					<a href="https://www.linkedin.com/in/s-m-kamrul-hasan/"><img src="./images/linkedin-logo.png" height="30px" style="margin-bottom:-3px"></a>
					<!--
					<a href="https://www.linkedin.com/in/lequan-yu-124811a2"><img src="./pic/LinkedIn_s.png" height="30px" style="margin-bottom:-3px"></a>
					<a href="https://zh-cn.facebook.com/people/Lequan-Yu/100003696557697"><img src="./pic/Facebook_s.png" height="30px" style="margin-bottom:-3px"></a>
					-->
				</p>
			</td>
			<td>
				<img src="./pic/git_image.jpg" border="0" width="260"><br>
			</td>
		</tr><tr>
	</tr></tbody>
</table>

<!--<h2>Biography [<a href="./CV-JinYueming.pdf">CV</a>]</h2>-->
<h2 style="color: black;">About Me</h2>
<p style="padding-bottom:30px;text-align:justify, "font-family:sans-serif">
	I finished my PhD from <a href="https://www.cis.rit.edu/" > Chester F. Carlson Center for Imaging Science</a> at Rochester Institute of Technology (RIT), Rochester, NY under the direction of my advisor, <a href="https://www.rit.edu/directory/calbme-cristian-linte" >Dr. Cristian Linte</a> and funded by both <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1808530&HistoricalAwards=false" >NSF</a> and NIH grants. My PhD Thesis was titled “From Fully-Supervised Single-Task to Semi-Supervised Multi-Task Deep Learning Architectures for Segmentation in Medical Imaging Applications”. I worked as an AI Research Intern at Philips Research in Cambridge, Massachusetts where I designed an extremely optimized object detection framework for object detection of COVID-19 features in ultrasound images (Ultrasound scans of the lung image) captured by Lumify portable Ultrasound probe. I worked for IBM Research in California as a Machine Learning Research Intern, where I've worked on deep neural network pruning/optimization for better explainable AI.
	</p>	
<h2 style="color: black;">Research Interests </h2>
<p style="padding-bottom:30px;text-align:justify" style="font-family:sans-serif">My research focuses broadly on developing and optimizing machine learning models for analyzing multi-modal images to enable more accurate automatic semantic and instance segmentation, 4D deformable registration, object detection, video object motion estimation, out-of-distribution (uncertainty) estimation, as well as video inpainting. I have profound knowledge of optimized label-efficient machine learning-based imaging problems. I have strong hands-on expertise in semi-/self-/un-supervised learning, representation learning, deep generative models, probabilistic Bayesian Monte Carlo models, and posterior estimation models.
	
<h2 style="color: black;">News</h2>

<ul>		  <li> <a style="font-family:sans-serif" style="color:blue;">[Jan. 2023]</a>  Successfully defended my PhD Dissertation Defense.</li>
		  <li> <a style="font-family:sans-serif" style="color:blue;">[May 2022]</a>   One paper got accepted to MIUA 2022.</li>	
		  <li> <a style="font-family:sans-serif" style="color:blue;">[May 2022]</a>   Got accepted to Oxford ML Summer School (~ 5% acceptance rate).</li>
		  <li> <a style="font-family:sans-serif" style="color:blue;">[Mar. 2022]</a>  Successfully Passed my Ph.D. Candidacy Exam.</li>	  
		  <li> <a style="font-family:sans-serif" style="color:blue;">[Oct. 2021]</a>  Attended the <a href="https://researchsummit.microsoft.com/home_public" style="color:blue"> Microsoft Research Summit 2021.</li>	  
	          <li> <a style="font-family:sans-serif" style="color:blue;">[Aug. 2021]</a>  Started Research Internship at <a href="https://www.philips.com/a-w/research/locations/cambridge-north-america.html" style="color:blue"> Philips, </a> Research North America, Cambridge, MA, USA.</li>	  
		  <li> <a style="font-family:sans-serif" style="color:blue;">[May. 2021]</a>  Got accepted to UCL Medical Image Computing Summer School (MedICSS), London</a>.</li>
		  <li> <a style="font-family:sans-serif" style="color:blue;">[Mar. 2021]</a>  Accepted offer as AI Research Intern at <a href="https://www.usa.philips.com/" style="color:blue"> Philips Research</a>, Cambridge, Massachusetts.</li>
	          <li> <a style="font-family:sans-serif" style="color:blue;">[Nov. 2020]</a>  Started Winter School at Center for Computational Medicine in Cardiology, Switzerland</a>.</li>
		  <li> <a style="font-family:sans-serif" style="color:blue;">[Oct. 2020]</a>  Paper got accepted at <a href="https://spie.org/conferences-and-exhibitions/medical-imaging?SSO=1" style="color:blue" > SPIE 2021,</a> San Diego, California.</li>
                  <li> <a style="font-family:sans-serif" style="color:blue;">[Aug. 2020]</a>  Received MICCAI Student Award as a part of NSF grant</a>.</li>
                  <li> <a style="font-family:sans-serif" style="color:blue;">[Aug. 2020]</a>  Started Research Internship at <a href="https://www.research.ibm.com/" style="color:blue"> IBM, </a> Almaden Research Center, San Jose, California.</li>
                  <li> <a style="font-family:sans-serif" style="color:blue;">[May. 2020]</a>  Accepted offer as Research Intern at <a href="https://www.ibm.com/us-en/" style="color:blue"> IBM </a>, San Jose, California.</li>
                  <li> <a style="font-family:sans-serif" style="color:blue;">[Apr. 2020]</a>  Presented paper at ISBI 2020</a>.</li>
                  <li> <a style="font-family:sans-serif" style="color:blue;">[Feb. 2020]</a>  Reviewer for MICCAI 2020.</a>.</li>
                  <li> <a style="font-family:sans-serif" style="color:blue;">[Nov. 2019]</a>  U-NetPlus paper accepted for oral presentation at RIT Graduate Showcase 2019</a>.</li>
<!-- 	<li>
		[12/2019] Paper on unpaired multi-modal learning was accepted by IEEE TMI.
	</li> -->
</ul>


<h2 style="color: black;">Professional Experience</h2>
<ul>
	<img align="right" src="./indexpics/philips.png" width="250px" class="right">
 	<li style="padding-bottom:25px;color: black;">
	<b> Philips Research </b><br>
	<a  style="color:gray"  > AI Research Intern 2021 </a> <br>
	<a  style="color:gray"  > Cambridge, Massachusetts </a> <br>
	<a  style="color:gray"  > Aug 2021 - Nov 2021 </a> <br>
	<br>
		<ul class="roman">
		 	<li>Designed an optimized (60FPS) lung features detector to improve PHILIPS LUMIFY handheld transducer</li>
		    <li>Demonstrated 38% more accuracy and had 86% fewer parameters compared to the SSD detection model</li>
		    <li>Deployed the framework (0.3 million) on an Android OS-based mobile platform for real-time inference</li>
		    <li>Delivered a semi-supervised feature detection system in the team from scratch for product development</li> 
		    <br>
		    <img src="./indexpics/lumify.webp" width="170px" class="center">
		    <img src="./indexpics/con.jpeg" width="170px" class="center">
        </ul>
    </li>

	<img align="right" src="./indexpics/ibm.png" width="320px" class="right">
 	<li style="padding-bottom:25px;color: black;">
	<b> IBM Research </b><br>
	<a  style="color:gray"  > Machine Learning Research Intern 2020 </a> <br>
	<a  style="color:gray"  > San Jose, California </a> <br>
	<a  style="color:gray"  > Aug 2020 - Nov 2020 </a> <br>
	<br>
		<ul class="roman">

		    <li>[Project 1] Prototype an explainable AutoML repository of subnetworks based on similarity and ranking algorithms having 82.9% fewer parameters and 28 times accurate than the baseline</li>
		 	<li>[Project 2] Restructured channel-wise pruning convolutional layers of transfer learning models on image classification tasks while achieving pruning ratios of up to 99.5% in parameters and 95.4% in FLOPs</li>
		    <li>Scripted more than 1,500 lines of code and delivered the project in the team on time</li>
		    <br>
		    <img src="./indexpics/IBM1.png" width="170px" class="center">
		    <img src="./indexpics/IBM2.jpeg" width="170px" class="center">
        </ul>
    </li>

    <img align="right"  src="./indexpics/OXF.png" width="90px" class="right">
 	<li style="padding-bottom:25px;color: black;">
	<b> University of Oxford </b><br>
	<a  style="color:gray"  > Machine Learning Summer School 2022 </a> <br>
	<a  style="color:gray"  > Oxford, United Kingdom </a> <br>
	<a  style="color:gray"  > Jun 2022 - Aug 2022 </a> <br>
	<br>
		<ul class="roman">

		    <li>Statistical Learning | Markov Decision Theory | Self-supervised Learning (Top 5%) </li>
        </ul>
    </li>


	<img align="right" src="./indexpics/rit.png" width="320px" class="right">
   	<li style="padding-bottom:25px;color: black;">
	<b> Rochester Institute of Technology </b><br>
	<a  style="color:gray"  > Research Assistant </a> <br>
	<a  style="color:gray"  > RIT Biomedical Modeling, Visualization and Image-guided Navigation Lab </a> <br>
	<a  style="color:gray"  > Rochester, NY </a> <br>
	<a  style="color:gray"  > Aug 2018 - Nov 2022 </a> <br>
	 <br>
	Advisor: <a href="https://www.rit.edu/directory/calbme-cristian-linte" style="color:blue"  > Cristian A. Linte, Ph.D</a><br>
		<ul class="roman">
		    <li> Tailored a Student-Teacher (gradient-to-gradient) augmentation-driven meta pseudo-labeling model by distilling knowledge through self-training, scaling up a 4.4% improvement in 3D semantic cardiac segmentation accuracy with a statistical hypothesis testing experiments on only 10% labeled data</li>
		    <li> Extracted noise-free 3D isosurface mesh with smoothing marching cubes algorithm and generated deformation field from a VoxelMorph-based 4D registration framework to predict cardiac motion which was four times faster than the baseline</li>  
		    <li> Acquired solid knowledge of imaging chain, including optics, sensors, ISP, and psycho-physical experiments with multi-view geometry of computer vision ranging from sensor fusion (LIDAR and Camera) to camera calibration (intrinsic and extrinsic parameter estimation, Epipolar geometry/stereo vision), image localization (edge, line, corner, and blob detection), perception, and statistical signal processing</li>
        </ul>
    </li>
	<p style="margin-top:3px"></p>	
	</li>
</ul>


<!-- <h2>Patent</h2>
<ul>
	<li>
		<a href="https://patentscope2.wipo.int/search/en/detail.jsf?docId=WO2017005591">Method and device for detecting pulmonary nodule in computed tomography image, and computer-readable storage medium</a>.<br>
		Qi Dou, <b>Quande Liu</b>, Hao Chen.<br>
		US Patent US20200005460A1, 2018.<br>
	</li>
</ul> -->

<!-- <h2>Selected Honors &amp; Awards</h2>
<table style="border-spacing:2px">
	
		<tbody>
		<tr><td> Microsoft Research Asia (MSRA) PhD Fellowship Nomination Award (2020) </td></tr>
		<tr><td> Outstanding Graduates Award, Zhejiang Province (2018)</td></tr>
		<tr><td> Zhejiang University Scholarship (2015-2017)</td></tr>
		<tr><td> Runner-up in 11th Robot Competition, Zhejiang University, 2017</td></tr>
		<tr><td> Second-class Scholarship for Outstanding Students, 2015-2017</td></tr>
		<tr><td> Second-class Academic Scholarship, 2015-2017</td></tr>
		<tr><td> Admission to Chu Kochen Honors College, Zhejiang University (2014)</td></tr>
	</tbody>
</table> -->

				 
<h2 style="color: black;">Software Development</h2>
<ul>
	<img align="right" src="./indexpics/mobile.png" width="350px" class="right">
 	<li style="padding-bottom:25px;color: black;">
	<b> NifTi File Viewer </b><br>
	<a  style="color:gray"  > macOS Compatible</a> <br>
	<br>
		<ul class="roman">
		 	<li>Built macOS software to view NIfTI files which is a type of file format for neuroimaging</li>
			<p>[<a href="https://github.com/SMKamrulHasan/smkamrulhasan.github.io/tree/main/data" style="color:blue" > Download Software </a>]</p>
		    <br>
		    <img src="./indexpics/NIFTI-VIEWER-LOGO.png" width="150px" class="center">
        </ul>
    </li>
											     
											     

<h2 style="color: black;" >Publications</h2>
<table id="tbPublications" width="100%">
	<tbody>

	* indicates equal contribution;	
	<tr>	
		<td width="270">
		<img src="./indexpics/cqsl.png" width="250px" class="center">
		</td style="text-align:justify">		    
	    
		<td> <font color="blue"> Learning Deep Representations of Cardiac Structures for 4D Cine MRI Image Segmentation through Semi-supervised Learning </font>  <br>
		<em> Applied Science </em>, 2022
		<strong></strong>
		<p>[<a href="https://link.springer.com/chapter/10.1007/978-3-031-12053-4_28" style="color:blue" >paper</a>][<a href="https://github.com/SMKamrulHasan/MTCTL" style="color:blue"  >code</a>][<a href="https://acdc.creatis.insa-lyon.fr/description/" style="color:blue" >dataset</a>][<a href="papers/2020/iteravg/ref.bib" style="color:blue" >bibtex</a>][<a href="https://youtu.be/shC0fjehT0k" style="color:blue" >Video 1</a>] </p>
		<p style="padding-bottom:30px;text-align:justify">
		In this paper, we propose a semi-supervised model --- namely, Combine-all in Semi-Supervised Learning (CqSL) --- to demonstrate the power of a simple combination of a disentanglement block, variational autoencoder (VAE), generative adversarial network (GAN), and a conditioning layer-based reconstructor for performing two important tasks in medical imaging: segmentation and reconstruction. Our work is motivated by the recent progress in image segmentation using semi-supervised learning (SSL), which has shown good results with limited labeled data and large amounts of unlabeled data.
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
		
		
	<tr>	
		<td width="270">
		<img src="./indexpics/organMNIST.png" width="250px" class="center">
		</td style="text-align:justify">		    
	    
		<td> <font color="blue"> The impact of class-dependent label noise in medical image (MedMNIST dataset) classification </font>  <br>
		<em> SPIE Medical Imaging -- Image Processing </em>, 2023
		<strong></strong>
		<p>[<a href="https://link.springer.com/chapter/10.1007/978-3-031-12053-4_28" style="color:blue" >paper</a>][<a href="https://github.com/SMKamrulHasan/MTCTL" style="color:blue"  >code</a>][<a href="https://acdc.creatis.insa-lyon.fr/description/" style="color:blue" >dataset</a>][<a href="papers/2020/iteravg/ref.bib" style="color:blue" >bibtex</a>][<a href="https://youtu.be/shC0fjehT0k" style="color:blue" >Video 1</a>] </p>
		<p style="padding-bottom:30px;text-align:justify">
		In this paper, we study this hypothesis using two publicly available datasets: a 2D organ classification dataset with target organ classes being visually distinct, and a histopathology image classification dataset where the target classes look very similar visually. Our results show that the label noise in one class has much higher impact on the model's performance on other classes for histopathology dataset compared to the organ dataset.
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
		
		
	<tr>	
		<td width="270">
		<img src="./indexpics/miua.gif" alt="this slowpoke moves"  width="250" alt="404 image"/>
		</td style="text-align:justify">		    
			    	    
			    
		<td> <font color="blue"> STAMP: A Self-training Student-Teacher Augmentation-driven Meta Pseudo-labeling Framework for 3D Cardiac MRI Image Segmentation </font>  <br>
		<strong>S. M. Kamrul Hasan</strong> and Cristian A. Linte. <br>
		<em> Medical Image Understanding and Analysis (MIUA) </em>, 2022
		<strong>oral</strong>
		<p>[<a href="https://link.springer.com/chapter/10.1007/978-3-031-12053-4_28" style="color:blue" >paper</a>][<a href="https://github.com/SMKamrulHasan/MTCTL" style="color:blue"  >code</a>][<a href="https://acdc.creatis.insa-lyon.fr/description/" style="color:blue" >dataset</a>][<a href="papers/2020/iteravg/ref.bib" style="color:blue" >bibtex</a>][<a href="https://youtu.be/shC0fjehT0k" style="color:blue" >Video 1</a>] </p>
		<p style="padding-bottom:30px;text-align:justify">
		The proposed method uses self-training (through meta pseudo-labeling) in concert with a Teacher network that instructs the Student network by generating pseudo-labels given unlabeled input data. Meta pseudo-labeling methods allow the Teacher network to constantly adapt in response to the performance of the Student network on the labeled dataset, hence enabling the Teacher to identify more effective pseudo-labels to instruct the Student. Moreover, to improve generalization and reduce error rate, we apply both strong and weak data augmentation policies, to ensure the segmentor outputs a consistent probability distribution regardless of the augmentation level.
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
	
		
	<tr>	
		<td width="270">
		<img src="./indexpics/embc.gif" alt="this slowpoke moves"  width="250" alt="404 image"/>
		<img src="./indexpics/embc22_2.png" width="250px" class="center">
		</td style="text-align:justify">		    
			    
		<td> <font color="blue"> Joint Segmentation and Uncertainty Estimation of Ventricular Structures from Cardiac MRI using Probability Calibration </font>  <br>
		<strong>S. M. Kamrul Hasan</strong> and Cristian A. Linte. <br>
		<em> International Conference of the Eng. in Med. & Bio (EMBC)</em>, 2022
		<strong>oral</strong>
		<p>[<a href="https://www.researchgate.net/publication/346026748_Segmentation_and_Removal_of_Surgical_Instruments_for_Background_Scene_Visualization_from_Endoscopic_Laparoscopic_Video" style="color:blue" >paper</a>][<a href="https://github.com/SMKamrulHasan/MTCTL" style="color:blue"  >code</a>][<a href="https://acdc.creatis.insa-lyon.fr/description/" style="color:blue" >dataset</a>][<a href="papers/2020/iteravg/ref.bib" style="color:blue" >bibtex</a>][<a href="https://youtu.be/shC0fjehT0k" style="color:blue" >Video 1</a>] </p>
		<p style="padding-bottom:30px;text-align:justify">
		In this work, we used a Bayesian version of our previously proposed CondenseUNet framework featuring both a learned group structure and a regularized weight-pruner to reduce the computational cost in volumetric image segmentation and help quantify predictive uncertainty. Our study further showcases the potential of our deep-learning framework to evaluate the correlation between the uncertainty and the segmentation errors for a given model. The proposed model was trained and tested on the Automated Cardiac Diagnosis Challenge (ACDC) dataset featuring 150 cine cardiac MRI patient dataset for the segmentation and uncertainty estimation of the left ventricle (LV), right ventricle (RV), and myocardium (Myo) at end-diastole (ED) and end-systole (ES) phases.
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
		
		
	<tr>	
		<td width="270">
		<img src="./indexpics/spie22.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		<img src="./indexpics/spie22_2.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		</td style="text-align:justify">		    
			    
		<td> <font color="blue">Calibration of cine MRI segmentation probability for uncertainty estimation using a Multi-Task Cross-Task Learning architecture.</font>  <br>
		<strong>S. M. Kamrul Hasan</strong> and Cristian A. Linte. <br>
		<em>SPIE Medical Imaging</em>, 2022
		<strong>oral</strong>
		<p>[<a href="https://spie.org/medical-imaging/presentation/Calibration-of-cine-MRI-segmentation-probability-for-uncertainty-estimation-using/12034-21" style="color:blue" >paper</a>][<a href="https://github.com/SMKamrulHasan/MTCTL" style="color:blue"  >code</a>][<a href="https://acdc.creatis.insa-lyon.fr/description/" style="color:blue" >dataset</a>][<a href="papers/2020/iteravg/ref.bib" style="color:blue" >bibtex</a>][<a href="https://youtu.be/shC0fjehT0k" style="color:blue" >Video 1</a>] </p>
		<p style="padding-bottom:30px;text-align:justify">
		In this work we propose a novel method that incorporates uncertainty estimation to detect failures in the segmentation masks generated by CNNs, our study further showcases the potential of our model to evaluate the correlation between the uncertainty and the segmentation errors for a given model. Furthermore, we introduce a multi-task Cross-task learning consistency approach to enforce the correlation between the pixel-level (segmentation) and the geometric-level (distance map) tasks. Our study serves as a proof-of-concept of how uncertainty measure correlates with the erroneous segmentation generated by different deep learning models, further showcasing the potential of our model to flag low-quality segmentation from a given model in our future study.
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
		
		
	<tr>	
		<td width="270">
		<img src="./indexpics/embc21.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		<img src="./indexpics/embc21_2.PNG" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		</td style="text-align:justify">		    
			    
		<td> <font color="blue">Motion Extraction of the Right Ventricle from 4D Cardiac Cine MRI Using A Deep Learning-Based Deformable Registration Framework.</font>  <br>
		Roshan Reddy Upendra*, <strong>S. M. Kamrul Hasan</strong>*, Richard Simon, Brian Jamison Wentz, Suzanne M. Shontz, Michael S. Sacks, and Cristian A. Linte. <br>
		"The first two authors share equal joint first authorship"<br>
		<em> International Conference of the Engineering in Medicine & Biology Society (EMBC)</em>, 2021,
		<strong>oral</strong>
		<p>[<a href="https://pubmed.ncbi.nlm.nih.gov/34892062/" style="color:blue" >paper</a>][<a href="https://github.com/SMKamrulHasan/RV_deformation" style="color:blue"  >code</a>][<a href="https://acdc.creatis.insa-lyon.fr/description/" style="color:blue" >dataset</a>][<a href="papers/2020/iteravg/ref.bib" style="color:blue" >bibtex</a>][<a href="https://youtu.be/shC0fjehT0k" style="color:blue" >Video 1</a>] </p>
		<p style="padding-bottom:30px;text-align:justify">
		In this work, we describe the development of dynamic patient-specific right ventricle (RV) models associated with normal subjects and abnormal RV patients to be subsequently used to assess RV function based on motion and kinematic analysis. In our study, we use a deep learning-based deformable network that takes 3D input volumes and outputs a motion field which is then used to generate isosurface meshes of the cardiac geometry at all cardiac frames by propagating the end-diastole (ED) isosurface mesh using the reconstructed motion field.
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
		
	<tr>	
		<td width="270">
		<img src="./images/cinc0.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		<img src="./images/cinc1.PNG" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		<img src="./images/cinc2.gif" alt="this slowpoke moves"  width="250" alt="404 image"/>
		</td style="text-align:justify">		    
			    
		<td> <font color="blue">A Multi-Task Cross-Task Learning Architecture for Ad-hoc Uncertainty Estimation in 3D Cardiac MRI Image Segmentation.</font>  <br>
		<strong>S. M. Kamrul Hasan</strong> and Cristian A. Linte. <br>
		<em>Computing in Cardiology </em>, 2021,
		<strong>oral</strong>
		<p>[<a href="https://ieeexplore.ieee.org/document/9662869" style="color:blue" >paper</a>][<a href="https://github.com/SMKamrulHasan/MTCTL" style="color:blue"  >code</a>][<a href="http://atriaseg2018.cardiacatlas.org/" style="color:blue" >dataset</a>][<a href="papers/2020/iteravg/ref.bib" style="color:blue" >bibtex</a>][<a href="https://youtu.be/shC0fjehT0k" style="color:blue" >Video 1</a>] </p>
		<p style="padding-bottom:30px;text-align:justify">
		To generate smoother and accurate segmentation masks from 3D cardiac MR images, we present a Multi-task Cross-task learning consistency approach to enforce the correlation between the pixel-level (segmentation) and the geometric-level (distance map) tasks. Our extensive experimentation with varied quantities of labeled data in the training sets justify the effectiveness of our model for the segmentation of left atrial cavity from Gadolinium-enhanced magnetic resonance (GE-MR) images. With the incorporation of uncertainty estimates to detect failures in the segmentation masks generated by CNNs, our study further showcases the potential of our model to flag low quality segmentation from a given model.
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
		
		
	<tr>	
		<td width="270">
		<img src="./indexpics/hasan8gf.gif" alt="this slowpoke moves"  width="250" alt="404 image"/>
		<img src="./indexpics/kamrul2.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		<img src="./indexpics/hasan6gf.gif" alt="this slowpoke moves"  width="250" alt="405 image"/>
		</td style="text-align:justify">		    
			    
		<td> <font color="blue">Segmentation and removal of surgical instruments for background scene visualization from Endoscopic / Laparoscopic video.</font>  <br>
		<strong>S. M. Kamrul Hasan</strong>, Richard A. Simon, and Cristian A. Linte. <br>
		<em>SPIE Medical Imaging</em>, 2021,
		<strong>oral</strong>
		<p>[<a href="https://www.researchgate.net/publication/346026748_Segmentation_and_Removal_of_Surgical_Instruments_for_Background_Scene_Visualization_from_Endoscopic_Laparoscopic_Video" style="color:blue" >paper</a>][<a href="https://github.com/SMKamrulHasan/Video_inpainting" style="color:blue"  >code</a>][<a href="https://endovis.grand-challenge.org/" style="color:blue" >dataset</a>][<a href="papers/2020/iteravg/ref.bib" style="color:blue" >bibtex</a>][<a href="https://youtu.be/shC0fjehT0k" style="color:blue" >Video 1</a>] [<a href="https://youtu.be/68kBjT60hkw" style="color:blue" >Video 2 </a>][<a href="https://youtu.be/--DnWQKsUHQ" style="color:blue" >Video 3</a>] [<a href="https://youtu.be/tIrRnB0K1b0" style="color:blue" >Video 4</a>] </p>
		<p style="padding-bottom:30px;text-align:justify">
		In this work, we implement a fully convolutional segmenter featuring both a learned group structure and a regularized weight-pruner to reduce the high computational cost in volumetric image segmentation. We validated our framework on the ACDC dataset featuring one healthy and four pathology groups imaged throughout the cardiac cycle. Based on these results, this technique has the potential to become an efficient and competitive cardiac image segmentation tool that may be used for cardiac computer-aided diagnosis, planning and guidance applications.
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
		
	<tr>
		<td width="270">
		<img src="./indexpics/RESULT2.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		<img src="./indexpics/PLOT1.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		</td style="text-align:justify">		
		<td> <font color="blue">L-CO-Net: Learned Condensation-Optimization Network for Clinical Parameter Estimation from Cardiac Cine MRI.</font>  <br>
		<b><strong>S. M. Kamrul Hasan</strong></b>, and Cristian A. Linte. <br>
        <em> International Conference of the Engineering in Medicine & Biology Society (EMBC)</em>, 2020,
        <strong>oral</strong>
		<p>[<a href="https://ieeexplore.ieee.org/document/9176491" style="color:blue"  >paper</a>][<a href="https://github.com/SMKamrulHasan/Regularized-Network" style="color:blue" >code</a>][<a href="https://www.creatis.insa-lyon.fr/Challenge/acdc/databases.html" style="color:blue" >dataset</a>][<a href="papers/2020/iteravg/ref.bib" style="color:blue" >bibtex</a>]</p>
        <p style="padding-bottom:30px;text-align:justify">
          In this work, we implement a fully convolutional segmenter featuring both a learned group structure and a regularized weight-pruner to reduce the high computational cost in volumetric image segmentation. We validated our framework on the ACDC dataset featuring one healthy and four pathology groups imaged throughout the cardiac cycle. Our technique achieved Dice scores of 96.8% (LV blood-pool), 93.3% (RV blood-pool) and 90.0% (LV Myocardium) with five-fold cross-validation and yielded similar clinical parameters as those estimated from the ground truth segmentation data. Based on these results, this technique has the potential to become an efficient and competitive cardiac image segmentation tool that may be used for cardiac computer-aided diagnosis, planning, and guidance applications.
        </p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
		
		
	<tr>
		<td width="270">
		<img src="./indexpics/model1.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		<img src="./indexpics/RESULT3.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		<img src="./indexpics/heart_main.gif" alt="this slowpoke moves"  width="250" alt="405 image"/>
		</td style="text-align:justify">		
		<td> <font color="blue">Learned Condensation-Optimization Network: A regularized Network for improved Cardiac Ventricles Segmentation on Breath-Hold Cine MRI.</font>  <br>
		<b><strong>S. M. Kamrul Hasan</strong></b>, and Cristian A. Linte. <br>
        <em> International Symposium on Biomedical Imaging (ISBI)</em>, 2020,
        <strong>oral</strong>
		<p>[<a href="https://www.researchgate.net/publication/340595489_Learned_Condensation-Optimization_Network_A_regularized_Network_for_improved_Cardiac_Ventricles_Segmentation_on_Breath-Hold_Cine_MRI" style="color:blue"  >paper</a>][<a href="https://github.com/SMKamrulHasan/Regularized-Network" style="color:blue" >code</a>][<a href="https://www.creatis.insa-lyon.fr/Challenge/acdc/databases.html" style="color:blue" >dataset</a>][<a href="papers/2020/iteravg/ref.bib" style="color:blue" >bibtex</a>]</p>
        <p style="padding-bottom:30px;text-align:justify">
         In this work, we implement a fully convolutional segmenter featuring both a learned group structure and a regularized weight-pruner to reduce the high computational cost in volumetric image segmentation. We validated the framework on the ACDC dataset and achieved accurate segmentation, leading to mean Dice scores of 96.80% (LV blood-pool), 93.33% (RV blood-pool), 90.0% (LV Myocardium) and yielded similar clinical parameters as those estimated from the ground-truth segmentation data.
        </p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
		
<!-- 	<tr>
		<td width="270">
		<img src="./indexpics/tmi20_cxr.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		</td>		
		<td> Deep Mining External Imperfect Data for Chest X-ray Disease Screening. <br>
		Luyang Luo, Lequan Yu, Hao Chen, <b>Quande Liu</b>, Xi Wang, Jiaqi Xu, Pheng-Ann Heng. <br>
		<em>IEEE Transactions on Medical Imaging (TMI)</em>, 2020.
		<p>[<a href="https://arxiv.org/pdf/2006.03796.pdf">paper</a>]</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
		 -->
	<tr>
		<td width="270">
		<img src="./indexpics/5.png" width="250px" style="box-shadow: 4px 4px 8px #888" >
		<img src="./indexpics/SPIE_2020_result.png" width="250px" style="box-shadow: 4px 4px 8px #888" >
		</td style="text-align:justify">		
		<td> <font color="blue">CondenseUNet: a memory-efficient condensely-connected architecture for bi-ventricular blood pool and myocardium segmentation.</font>  <br>
		<b><strong>S. M. Kamrul Hasan</strong></b>, and Cristian A. Linte. <br>
       		<em>SPIE Medical Imaging</em>, 2020,
		<strong>oral</strong>
		<p>[<a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11315/113151J/CondenseUNet---a-memory-efficient-condensely-connected-architecture-for/10.1117/12.2550640.short?SSO=1" style="color:blue"  >paper</a>][<a href="https://github.com/SMKamrulHasan/CondenseUNet" style="color:blue" >code</a>][<a href="https://www.creatis.insa-lyon.fr/Challenge/acdc/databases.html" style="color:blue" >dataset</a>][<a href="papers/2020/iteravg/ref.bib" style="color:blue" >bibtex</a>]</p>
        <p style="padding-bottom:30px;text-align:justify">
        In this work, we propose a novel memory-efficient Convolutional Neural Network (CNN) architecture as a modification of both CondenseNet, as well as DenseNet for ventricular blood-pool segmentation by introducing a bottleneck block and an upsampling path. Our experiments show that the proposed architecture runs on the Automated Cardiac Diagnosis Challenge (ACDC) dataset using half (50%) the memory requirement of DenseNet and one-twelfth (∼ 8%) of the memory requirements of U-Net, while still maintaining excellent accuracy of cardiac segmentation.
        </p>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
		
	<tr>
		<td width="270">
		<img src="./indexpics/cinc.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		</td style="text-align:justify">		
		<td> <font color="blue">Toward Quantification and Visualization of Active Stress Waves for Myocardial Biomechanical Function Assessment.</font>  <br>
        Niels F Otani, Dylan Dang, Christopher Beam, Fariba Mohammadi, Brian Wentz, <strong>S. M. Kamrul Hasan</strong>, Suzanne M Shontz, Karl Q Schwarz, Sabu Thomas, and Cristian A. Linte. <br>
		<em>Computing in Cardiology (CinC)</em>, 2019.
		<p>[<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7373340/" style="color:blue" >paper</a>][<a href="https://github.com/SMKamrulHasan" style="color:blue" >code</a>][<a href="" style="color:blue"  >dataset</a>][<a href="papers/2020/iteravg/ref.bib" style="color:blue" >bibtex</a>]</p>
        <p style="padding-bottom:30px;text-align:justify">
         In the forward model, tissue deformation was generated using a test wave with active stresses that mimic the myocardial contractile forces. The generated deformation field was used as input to an inverse model designed to reconstruct the original active stress distribution. We numerically simulated malfunctioning tissue regions (experiencing limited contractility and hence active stress) within the healthy tissue. We also assessed model sensitivity by adding noise to the deformation field generated using the forward model. The difference image between the original and reconstructed active stress distribution suggests that the model accurately estimates active stress from tissue deformation data with a high signal-to-noise ratio.
        </p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
		
	<tr>
		<td width="270">
		<img src="./indexpics/EMBC_2019_result.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		<img src="./indexpics/EMBC_2019_result2.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		<img src="./indexpics/EMBC_2019_result3.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		<img src="./indexpics/parts.gif" alt="this slowpoke moves"  width="250" alt="404 image"/>
		</td style="text-align:justify">		
		<td> <font color="blue">U-NetPlus: A Modified Encoder-Decoder U-Net Architecture for Semantic and Instance Segmentation of Surgical Instruments from Laparoscopic Images.</font>  <br>
		<strong>S. M. Kamrul Hasan</strong>, and Cristian A. Linte. <br>
		<em>International Conference of the IEEE Engineering in Medicine and Biology (EMBC)</em>, 2020,
        <strong>oral</strong>
		<p>[<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7372295/" style="color:blue" >paper</a>][<a href="unetplus.github.io" style="color:blue" >code</a>][<a href="https://endovis.grand-challenge.org/" style="color:blue" >dataset</a>][<a href="papers/2020/iteravg/ref.bib" style="color:blue" >bibtex</a>]</p>
        <p style="padding-bottom:30px;text-align:justify">
         In this work, we modify the U-Net architecture by introducing a pre-trained encoder and re-design the decoder part, by replacing the transposed convolution operation with an upsampling operation based on nearest-neighbor (NN) interpolation. To further improve performance, we also employ a very fast and flexible data augmentation technique. We trained the framework on 8 x 225 frame sequences of robotic surgical videos available through the MICCAI 2017 EndoVis Challenge dataset and tested it on 8 x 75 frame and 2 x 300 frame videos. Using our U-NetPlus architecture, we report a 90.20\% DICE for binary segmentation, 76.26% DICE for instrument part segmentation, and 46.07% for instrument type (i.e., all instruments) segmentation, outperforming the results of previous techniques implemented and tested on these data.
        </p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>
		<td width="270">
		<img src="./indexpics/WNYISPW_2018_model.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		<img src="./indexpics/WNYISPW_2018_nnret.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
			
			
		</td style="text-align:justify">		
		<td> <font color="blue">A Modified U-Net Convolutional Network Featuring a Nearest-neighbor Re-sampling-based Elastic-Transformation for Brain Tissue Characterization and Segmentation.</font>  <br>

        <strong>S. M. Kamrul Hasan</strong>, and Cristian A. Linte. <br>
		<em> Western New York Image and Signal Processing Workshop (WNYISPW)</em>, 2018,
        <strong>oral</strong>
		<p>[<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6583803/" style="color:blue" >paper</a>][<a href="https://github.com/SMKamrulHasan/" style="color:blue" >code</a>][<a href="https://www.med.upenn.edu/sbia/brats2017/data.html" style="color:blue" >dataset</a>][<a href="papers/2020/iteravg/ref.bib" style="color:blue" >bibtex</a>]</p>
        <p style="padding-bottom:30px;text-align:justify">
         Though this model works better on BRATS 2015 dataset by using pixel-wise segmentation map of the input image like an auto-encoder which assures best segmentation accuracy, but it is not correct for all the cases. So, I have planned to improve this U-net model by replacing the de-convolution part with the upsampled by Nearest-neighbor algorithm and also by using elastic transformation for increasing the training dataset to make the model more robust on Low graded tumor. I had trained my NNRET U-net model on BRATS 2017 dataset and got a better performance than the state of the art classic U-net model.
        </p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>


</tbody></table>

<h2 style="color: black;">Talks</h2>
<table id="tbTeaching" border="0" width="100%">
	<tbody>
		<tr>
			<td> 2022</td><td>Podcast</td><td>Bangladeshi Researchers in Data Science and Machine Learning</td>
		</tr>
		<tr>
			<td> 2021</td><td>Presentation</td><td>Philips Research North America</td>
		</tr>
		<tr>
			<td> 2021</td><td>Workshop</td><td>RIT Co-op Placement (Guest Speaker)</td>
		</tr>		
		<tr>
			<td> 2020</td><td>Presentation</td><td>IBM Almaden Research Center</td>
		</tr>
	</tbody>
</table>
	
<h2 style="color: black;">Honors &amp; Awards</h2>
<ul>
	<li>
		<tr><td> MICCAI student travel award as a part of NSF Grant (2020) </td></tr>
	</li>
	<li>
		<tr><td> <a href="https://ewh.ieee.org/r1/rochester/sp/WNYISPW2018.html" style="color:blue" > Best paper award </a>, Western New York Image and Signal Processing Workshop (2018)</td></tr>
	</li>
	<li>
		<tr><td> Imagine Festival RIT Award from KODAK (2017)</td></tr>
	</li>
	<li>
		<tr><td>RIT Graduate Scholarship (2017)</td></tr>
	</li>
	<li>
		<tr><td>Awarded for achieving GPAs of 3.85∼4.0 in total of six out of eight semesters (2012-2015)</td></tr>
	</li>
		<!-- <tr><td> Runner-up in 11th Robot Competition, Zhejiang University, 2017</td></tr> -->
		<!-- <tr><td> Second-class Scholarship for Outstanding Students, 2015-2017</td></tr> -->
		<!-- <tr><td> Second-class Academic Scholarship, 2015-2017</td></tr> -->
</ul>

<h2 style="color: black;">Reviewing</h2>
<ul>
	<li style="padding-bottom:30px;" >	
	<b>Conference and Journal Reviews:</b><br>
	Scientific Report (Nature) 2022 <br>
	NeurIPS 2020 <br>
	MICCAI 2020<br>
	IEEE Access 2019<br>
	IJCARS 2020<br>
	IPCAI 2020<br>
	<!-- IEEE Winter Conference on Applications of Computer Vision (WACV) 2020 <br> -->
	</li>	
	<p style="margin-top:3px"></p>		
</ul>

	
<!-- <h2>Teaching</h2>
<table id="tbTeaching" border="0" width="100%">
	<tbody>
		<tr>
			<td> 2019-2020</td><td>Spring</td><td>Principles of Programming Languages (CSCI 3180)</td>
		</tr>
		<tr>
			<td> 2019-2020</td><td>Fall</td><td>Problem Solving by Programming (ENGG 1110)</td>
		</tr>
		<tr>
			<td> 2018-2019</td><td>Spring</td><td>Problem Solving by Programming (ENGG 1110)</td>
		</tr>
		<tr>
			<td> 2018-2019</td><td>Fall</td><td>Digital Logic and Systems (ENGG 2020)</td>
		</tr>
	</tbody>
</table> -->

<!--

<h2>Experience</h2>
<li>
	Research Assistant, &nbsp 08. 2015 - Now
	<p></p>
	<p>&nbsp&nbsp&nbsp The Chinese University of Hong Kong</p>
	<p></p> 
		<p>&nbsp&nbsp&nbsp Advisor: Pheng Ann Heng</p> 
        <p style="margin-top:3px">
		</p>
</li>
<li>
	Software Engineering Intern, &nbsp 01. 2015 - 04. 2015
	<p></p>
	<p>&nbsp&nbsp&nbsp Epiclouds, a startup company in Hangzhou</p>
	<p></p> 
        <p style="margin-top:3px">
		</p>
</li>

	-->


<div id="footer">
	<div id="footer-text"></div>
</div>
	<p><center>
	<td width="270">
	<img src="./indexpics/scanme.JPG" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
	</td style="text-align:justify">
	</div> 
	<!--	
      	<div id="clustrmaps-widget" style="width:40%">
	500	<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=LJNWkxUAFjdgZdHhjWvEOF1K9cIg45om0jzghCyXpkc&cl=ffffff&w=a"></script>
	</div>  
	-->
	<br><center>
        &copy; S. M. Kamrul Hasan | Last updated: June 2022
     
      </center></p>


</div>

</body></html>
